# ==============================================================================
# Merged Lakeflow Source: microsoft_teams
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator
import json
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect(ABC):
        """Base interface that each source connector must implement.

        Subclass this and implement all abstract methods to create a connector that
        integrates with the community connector library and ingestion pipeline.
        """

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """
            self.options = options

        @abstractmethod
        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        @abstractmethod
        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        @abstractmethod
        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        @abstractmethod
        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records and an offset.

            The framework calls this method repeatedly to paginate through data.
            start_offset is None only on the very first call of the very first run
            of a connector. On subsequent runs the framework resumes from the last
            checkpointed offset, so start_offset will already be populated. Each
            call returns (records, end_offset). The framework passes end_offset as
            start_offset to the next call. Pagination stops when the returned
            offset equals start_offset (i.e., no more data).

            For tables that cannot be incrementally read, return None as the offset to
            read the entire table in one batch. Non-checkpointable synthetic offsets can
            be used to split the data into multiple batches.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from. None only on the
                    first call of the first run; on subsequent runs it carries the
                    checkpointed offset from the previous run.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of records as JSON-compatible dicts. Do NOT convert
                    values according to get_table_schema(); the framework handles that.
                offset: A dict representing the position after this batch.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            This method follows the same pagination and offset protocol as read_table:
            the framework calls it repeatedly, passing the previous end_offset as
            start_offset, until the returned offset equals start_offset.

            Override this method if any of your tables use ingestion_type
            "cdc_with_deletes". The default implementation raises NotImplementedError.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: A dict (same format as read_table).
            """
            raise NotImplementedError(
                "read_table_deletes() must be implemented when ingestion_type is 'cdc_with_deletes'"
            )


    ########################################################
    # src/databricks/labs/community_connector/sources/microsoft_teams/microsoft_teams_schemas.py
    ########################################################

    IDENTITY_SET_SCHEMA = StructType(
        [
            StructField("application", StringType(), True),
            StructField("device", StringType(), True),
            StructField(
                "user",
                StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("displayName", StringType(), True),
                        StructField("userIdentityType", StringType(), True),
                    ]
                ),
                True,
            ),
        ]
    )
    """Nested identity set schema used for message senders and mentions."""

    BODY_SCHEMA = StructType(
        [
            StructField("contentType", StringType(), True),
            StructField("content", StringType(), True),
        ]
    )
    """Message body schema with content type and content."""

    ATTACHMENT_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("contentType", StringType(), True),
            StructField("contentUrl", StringType(), True),
            StructField("content", StringType(), True),
            StructField("name", StringType(), True),
            StructField("thumbnailUrl", StringType(), True),
        ]
    )
    """Attachment schema for message attachments."""

    MENTION_SCHEMA = StructType(
        [
            StructField("id", LongType(), True),
            StructField("mentionText", StringType(), True),
            StructField("mentioned", IDENTITY_SET_SCHEMA, True),
        ]
    )
    """Mention schema for @mentions in messages."""

    REACTION_SCHEMA = StructType(
        [
            StructField("reactionType", StringType(), True),
            StructField("createdDateTime", StringType(), True),
            StructField("user", IDENTITY_SET_SCHEMA, True),
        ]
    )
    """Reaction schema for emoji reactions on messages."""

    CHANNEL_IDENTITY_SCHEMA = StructType(
        [
            StructField("teamId", StringType(), True),
            StructField("channelId", StringType(), True),
        ]
    )
    """Channel identity schema for message channel references."""


    # =============================================================================
    # Table Schema Definitions
    # =============================================================================

    TEAMS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("displayName", StringType(), True),
            StructField("description", StringType(), True),
            StructField("classification", StringType(), True),
            StructField("visibility", StringType(), True),
            StructField("webUrl", StringType(), True),
            StructField("isArchived", BooleanType(), True),
            StructField("createdDateTime", StringType(), True),
            StructField("internalId", StringType(), True),
            StructField("tenantId", StringType(), True),
            StructField("specialization", StringType(), True),
            StructField("memberSettings", StringType(), True),
            StructField("guestSettings", StringType(), True),
            StructField("messagingSettings", StringType(), True),
            StructField("funSettings", StringType(), True),
        ]
    )
    """Schema for the teams table."""

    CHANNELS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("team_id", StringType(), False),
            StructField("displayName", StringType(), True),
            StructField("description", StringType(), True),
            StructField("email", StringType(), True),
            StructField("webUrl", StringType(), True),
            StructField("membershipType", StringType(), True),
            StructField("createdDateTime", StringType(), True),
            StructField("isFavoriteByDefault", BooleanType(), True),
            StructField("isArchived", BooleanType(), True),
            StructField("tenantId", StringType(), True),
        ]
    )
    """Schema for the channels table."""

    MESSAGES_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("team_id", StringType(), False),
            StructField("channel_id", StringType(), False),
            StructField("replyToId", StringType(), True),
            StructField("etag", StringType(), True),
            StructField("messageType", StringType(), True),
            StructField("createdDateTime", StringType(), True),
            StructField("lastModifiedDateTime", StringType(), True),
            StructField("lastEditedDateTime", StringType(), True),
            StructField("deletedDateTime", StringType(), True),
            StructField("subject", StringType(), True),
            StructField("summary", StringType(), True),
            StructField("importance", StringType(), True),
            StructField("locale", StringType(), True),
            StructField("webUrl", StringType(), True),
            StructField("from", IDENTITY_SET_SCHEMA, True),
            StructField("body", BODY_SCHEMA, True),
            StructField("attachments", ArrayType(ATTACHMENT_SCHEMA), True),
            StructField("mentions", ArrayType(MENTION_SCHEMA), True),
            StructField("reactions", ArrayType(REACTION_SCHEMA), True),
            StructField("channelIdentity", CHANNEL_IDENTITY_SCHEMA, True),
            StructField("policyViolation", StringType(), True),
            StructField("eventDetail", StringType(), True),
            StructField("messageHistory", StringType(), True),
        ]
    )
    """Schema for the messages table."""

    MEMBERS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("team_id", StringType(), False),
            StructField("roles", ArrayType(StringType()), True),
            StructField("displayName", StringType(), True),
            StructField("userId", StringType(), True),
            StructField("email", StringType(), True),
            StructField("visibleHistoryStartDateTime", StringType(), True),
            StructField("tenantId", StringType(), True),
        ]
    )
    """Schema for the members table."""

    MESSAGE_REPLIES_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("parent_message_id", StringType(), False),
            StructField("team_id", StringType(), False),
            StructField("channel_id", StringType(), False),
            StructField("replyToId", StringType(), True),
            StructField("etag", StringType(), True),
            StructField("messageType", StringType(), True),
            StructField("createdDateTime", StringType(), True),
            StructField("lastModifiedDateTime", StringType(), True),
            StructField("lastEditedDateTime", StringType(), True),
            StructField("deletedDateTime", StringType(), True),
            StructField("subject", StringType(), True),
            StructField("summary", StringType(), True),
            StructField("importance", StringType(), True),
            StructField("locale", StringType(), True),
            StructField("webUrl", StringType(), True),
            StructField("from", IDENTITY_SET_SCHEMA, True),
            StructField("body", BODY_SCHEMA, True),
            StructField("attachments", ArrayType(ATTACHMENT_SCHEMA), True),
            StructField("mentions", ArrayType(MENTION_SCHEMA), True),
            StructField("reactions", ArrayType(REACTION_SCHEMA), True),
            StructField("channelIdentity", CHANNEL_IDENTITY_SCHEMA, True),
            StructField("policyViolation", StringType(), True),
            StructField("eventDetail", StringType(), True),
            StructField("messageHistory", StringType(), True),
        ]
    )
    """Schema for the message_replies table."""


    # =============================================================================
    # Schema Mapping
    # =============================================================================

    TABLE_SCHEMAS: dict[str, StructType] = {
        "teams": TEAMS_SCHEMA,
        "channels": CHANNELS_SCHEMA,
        "messages": MESSAGES_SCHEMA,
        "members": MEMBERS_SCHEMA,
        "message_replies": MESSAGE_REPLIES_SCHEMA,
    }
    """Mapping of table names to their StructType schemas."""


    # =============================================================================
    # Table Metadata Definitions
    # =============================================================================

    TABLE_METADATA: dict[str, dict] = {
        "teams": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
            "endpoint": "groups?$filter=resourceProvisioningOptions/Any(x:x eq 'Team')",
        },
        "channels": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
            "endpoint": "teams/{team_id}/channels",
        },
        "messages": {
            "primary_keys": ["id"],
            "cursor_field": "lastModifiedDateTime",
            "ingestion_type": "cdc",
            "endpoint": "teams/{team_id}/channels/{channel_id}/messages",
        },
        "members": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
            "endpoint": "teams/{team_id}/members",
        },
        "message_replies": {
            "primary_keys": ["id"],
            "cursor_field": "lastModifiedDateTime",
            "ingestion_type": "cdc",
            "endpoint": "teams/{team_id}/channels/{channel_id}/messages/{message_id}/replies",
        },
    }
    """Metadata for each table including primary keys, cursor field, and ingestion type."""


    # =============================================================================
    # Supported Tables
    # =============================================================================

    SUPPORTED_TABLES: list[str] = list(TABLE_SCHEMAS.keys())
    """List of all table names supported by the Microsoft Teams connector."""


    ########################################################
    # src/databricks/labs/community_connector/sources/microsoft_teams/microsoft_teams_utils.py
    ########################################################

    class MicrosoftGraphClient:
        """HTTP client for Microsoft Graph API with OAuth 2.0 and retry logic."""

        def __init__(self, tenant_id: str, client_id: str, client_secret: str) -> None:
            self.tenant_id = tenant_id
            self.client_id = client_id
            self.client_secret = client_secret
            self.base_url = "https://graph.microsoft.com/v1.0"
            self._access_token = None
            self._token_expiry = None

        def get_access_token(self) -> str:
            """
            Acquire OAuth 2.0 access token using client credentials flow.
            Tokens are cached and reused until 5 minutes before expiration.
            """
            if not self.tenant_id or not self.client_id or not self.client_secret:
                raise ValueError(
                    "Missing required options: tenant_id, client_id, and client_secret are required. "
                    "Pass them in the connection properties or in table_configuration for each table."
                )

            if (
                self._access_token
                and self._token_expiry
                and datetime.utcnow() < self._token_expiry
            ):
                return self._access_token

            token_url = (
                f"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token"
            )

            data = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "scope": "https://graph.microsoft.com/.default",
                "grant_type": "client_credentials",
            }

            try:
                response = requests.post(token_url, data=data, timeout=30)
                if response.status_code != 200:
                    if self.tenant_id and len(self.tenant_id) > 8:
                        tenant_preview = f"{self.tenant_id[:8]}..."
                    else:
                        tenant_preview = "INVALID"
                    raise RuntimeError(
                        f"Token acquisition failed: {response.status_code}\n"
                        f"URL: {token_url}\n"
                        f"Tenant ID (first 8 chars): {tenant_preview}\n"
                        f"Response: {response.text[:500]}"
                    )

                token_data = response.json()
                self._access_token = token_data["access_token"]

                expires_in = token_data.get("expires_in", 3600)
                self._token_expiry = datetime.utcnow() + timedelta(
                    seconds=expires_in - 300
                )

                return self._access_token

            except requests.RequestException as e:
                raise RuntimeError(f"Token acquisition request failed: {str(e)}")

        def make_request(  # pylint: disable=too-many-branches
            self, url: str, params: dict = None, max_retries: int = 3
        ) -> dict:
            """
            Make HTTP GET request to Microsoft Graph API with exponential backoff retry.
            Handles rate limiting (429), server errors, and token refresh on 401.
            """
            for attempt in range(max_retries):
                try:
                    headers = {
                        "Authorization": f"Bearer {self.get_access_token()}",
                        "Content-Type": "application/json",
                    }

                    response = requests.get(url, params=params, headers=headers, timeout=30)

                    if response.status_code == 200:
                        return response.json()

                    elif response.status_code == 401:
                        self._access_token = None
                        self._token_expiry = None
                        if attempt < max_retries - 1:
                            continue
                        raise RuntimeError(
                            f"Authentication failed (401). Please verify credentials and permissions."
                        )

                    elif response.status_code == 403:
                        raise RuntimeError(
                            f"Forbidden (403). Please verify the app has required permissions: {response.text}"
                        )

                    elif response.status_code == 404:
                        raise RuntimeError(
                            f"Resource not found (404). Please verify team_id/channel_id: {response.text}"
                        )

                    elif response.status_code == 429:
                        retry_after = int(response.headers.get("Retry-After", 60))
                        time.sleep(retry_after)
                        continue

                    elif response.status_code in [500, 502, 503]:
                        if attempt < max_retries - 1:
                            time.sleep(2**attempt)
                            continue
                        raise RuntimeError(
                            f"Server error ({response.status_code}) after {max_retries} retries: {response.text}"
                        )

                    else:
                        raise RuntimeError(
                            f"Request failed with status {response.status_code}: {response.text}"
                        )

                except requests.RequestException as e:
                    if attempt < max_retries - 1:
                        time.sleep(2**attempt)
                        continue
                    if isinstance(e, requests.Timeout):
                        raise RuntimeError(
                            f"Request timeout after {max_retries} attempts: {url}"
                        ) from e
                    raise RuntimeError(f"Request exception: {str(e)}") from e

            raise RuntimeError(f"Max retries ({max_retries}) exceeded for: {url}")


    def fetch_all_team_ids(client: MicrosoftGraphClient, max_pages: int) -> list[str]:
        """Fetch all team IDs from the organization."""
        teams_url = f"{client.base_url}/groups?$filter=resourceProvisioningOptions/Any(x:x eq 'Team')"
        teams_params = {"$select": "id"}
        team_ids = []
        pages_fetched = 0
        next_url: str | None = teams_url

        while next_url and pages_fetched < max_pages:
            if pages_fetched == 0:
                data = client.make_request(teams_url, params=teams_params)
            else:
                data = client.make_request(next_url)

            teams = data.get("value", [])
            for team in teams:
                team_id = team.get("id")
                if team_id:
                    team_ids.append(team_id)

            next_url = data.get("@odata.nextLink")
            pages_fetched += 1

            if next_url:
                time.sleep(0.1)

        return team_ids


    def fetch_all_channel_ids(
        client: MicrosoftGraphClient, team_id: str, max_pages: int
    ) -> list[str]:
        """Fetch all channel IDs for a specific team. Returns empty list if team is inaccessible."""
        channels_url = f"{client.base_url}/teams/{team_id}/channels"
        channels_params = {"$select": "id"}
        channel_ids = []
        pages_fetched = 0
        next_url: str | None = channels_url

        try:
            while next_url and pages_fetched < max_pages:
                if pages_fetched == 0:
                    data = client.make_request(channels_url, params=channels_params)
                else:
                    data = client.make_request(next_url)

                channels = data.get("value", [])
                for channel in channels:
                    channel_id = channel.get("id")
                    if channel_id:
                        channel_ids.append(channel_id)

                next_url = data.get("@odata.nextLink")
                pages_fetched += 1

                if next_url:
                    time.sleep(0.1)
        except Exception as e:
            if "404" in str(e) or "403" in str(e):
                return []
            raise

        return channel_ids


    def fetch_all_message_ids(
        client: MicrosoftGraphClient,
        team_id: str,
        channel_id: str,
        max_pages: int,
    ) -> list[str]:
        """Fetch all message IDs for a specific channel. Returns empty list if channel is inaccessible."""
        messages_url = f"{client.base_url}/teams/{team_id}/channels/{channel_id}/messages"
        messages_params = {"$top": 50}
        message_ids = []
        pages_fetched = 0
        next_url: str | None = messages_url

        try:
            while next_url and pages_fetched < max_pages:
                if pages_fetched == 0:
                    data = client.make_request(messages_url, params=messages_params)
                else:
                    data = client.make_request(next_url)

                messages = data.get("value", [])
                for message in messages:
                    message_id = message.get("id")
                    if message_id:
                        message_ids.append(message_id)

                next_url = data.get("@odata.nextLink")
                pages_fetched += 1

                if next_url:
                    time.sleep(0.1)
        except Exception as e:
            if "404" in str(e) or "403" in str(e):
                return []
            raise

        return message_ids


    def serialize_complex_fields(record: dict[str, Any], fields: list[str]) -> None:
        """Serialize complex objects (dict/list) to JSON strings in-place."""
        for field in fields:
            if field in record:
                val = record[field]
                if isinstance(val, (dict, list)):
                    record[field] = json.dumps(val)


    def parse_int_option(
        table_options: dict[str, str], key: str, default: int
    ) -> int:
        """Parse an integer option from table_options with a default value."""
        try:
            return int(table_options.get(key, default))
        except (TypeError, ValueError):
            return default


    def compute_next_cursor(
        max_modified: str | None,
        current_cursor: str | None,
        lookback_seconds: int,
    ) -> str | None:
        """Compute the next cursor value with a lookback window."""
        if not max_modified:
            return current_cursor

        try:
            dt = datetime.fromisoformat(max_modified.replace("Z", "+00:00"))
            dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
            return dt_with_lookback.isoformat().replace("+00:00", "Z")
        except Exception:
            return max_modified


    def get_cursor_from_offset(
        start_offset: dict | None, table_options: dict[str, str]
    ) -> str | None:
        """Extract the cursor value from start_offset or fall back to table_options."""
        cursor = None
        if start_offset and isinstance(start_offset, dict):
            cursor = start_offset.get("cursor")
        if not cursor:
            cursor = table_options.get("start_date")
        return cursor


    def resolve_team_ids(
        client: MicrosoftGraphClient,
        table_options: dict[str, str],
        table_name: str,
        max_pages: int,
    ) -> list[str]:
        """Resolve team IDs from table_options or auto-discover all teams."""
        team_id = table_options.get("team_id")
        fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

        if not team_id and not fetch_all_teams:
            raise ValueError(
                f"table_options for '{table_name}' must include either 'team_id' "
                f"or 'fetch_all_teams=true'"
            )

        if fetch_all_teams:
            return fetch_all_team_ids(client, max_pages)
        return [team_id]


    def resolve_team_channel_pairs(
        client: MicrosoftGraphClient,
        table_options: dict[str, str],
        table_name: str,
        max_pages: int,
    ) -> list[tuple[str, str]]:
        """Resolve team-channel pairs from table_options or auto-discover."""
        team_ids = resolve_team_ids(client, table_options, table_name, max_pages)
        channel_id = table_options.get("channel_id")
        fetch_all_channels = table_options.get("fetch_all_channels", "").lower() == "true"

        if not channel_id and not fetch_all_channels:
            raise ValueError(
                f"table_options for '{table_name}' must include either 'channel_id' "
                f"or 'fetch_all_channels=true'"
            )

        pairs = []
        for tid in team_ids:
            if fetch_all_channels:
                ch_ids = fetch_all_channel_ids(client, tid, max_pages)
                if not ch_ids:
                    continue
            else:
                ch_ids = [channel_id]
            for ch_id in ch_ids:
                pairs.append((tid, ch_id))
        return pairs


    ########################################################
    # src/databricks/labs/community_connector/sources/microsoft_teams/microsoft_teams.py
    ########################################################

    _TEAM_SETTINGS_FIELDS = [
        "memberSettings", "guestSettings",
        "messagingSettings", "funSettings",
    ]

    # Fields that need JSON serialization in message/reply records
    _MESSAGE_COMPLEX_FIELDS = [
        "policyViolation", "eventDetail", "messageHistory",
    ]


    def _build_deleted_record(
        record_id: str, team_id: str, channel_id: str,
    ) -> dict[str, Any]:
        """Build a record representing a deleted entity."""
        return {
            "id": record_id,
            "team_id": team_id,
            "channel_id": channel_id,
            "_deleted": True,
            "lastModifiedDateTime": (
                datetime.now().isoformat().replace("+00:00", "Z")
            ),
        }


    def _resolve_message_triples(
        client: MicrosoftGraphClient,
        pairs: list[tuple[str, str]],
        table_options: dict[str, str],
        max_pages: int,
    ) -> list[tuple[str, str, str]]:
        """Resolve (team_id, channel_id, message_id) triples."""
        message_id = table_options.get("message_id")
        fetch_all = (
            table_options.get("fetch_all_messages", "").lower() == "true"
        )

        if not message_id and not fetch_all:
            raise ValueError(
                "table_options for 'message_replies' must include "
                "either 'message_id' or 'fetch_all_messages=true'"
            )

        triples = []
        for tid, cid in pairs:
            if fetch_all:
                msg_ids = fetch_all_message_ids(
                    client, tid, cid, max_pages,
                )
                if not msg_ids:
                    continue
            else:
                msg_ids = [message_id]
            for mid in msg_ids:
                triples.append((tid, cid, mid))
        return triples


    class MicrosoftTeamsLakeflowConnect(LakeflowConnect):
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize Microsoft Teams connector with OAuth 2.0 credentials.

            Required options (stored in UC Connection properties):
              - tenant_id: Azure AD tenant ID
              - client_id: Application (client) ID
              - client_secret: Client secret value
            """
            self._client = MicrosoftGraphClient(
                tenant_id=options.get("tenant_id"),
                client_id=options.get("client_id"),
                client_secret=options.get("client_secret"),
            )

        def list_tables(self) -> list[str]:
            return SUPPORTED_TABLES

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            if table_name not in TABLE_SCHEMAS:
                raise ValueError(
                    f"Unsupported table: {table_name}. "
                    f"Supported tables: {SUPPORTED_TABLES}"
                )
            return TABLE_SCHEMAS[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            if table_name not in TABLE_METADATA:
                raise ValueError(
                    f"Unsupported table: {table_name}. "
                    f"Supported tables: {SUPPORTED_TABLES}"
                )

            config = TABLE_METADATA[table_name]
            metadata = {
                "primary_keys": config["primary_keys"],
                "ingestion_type": config["ingestion_type"],
            }
            if "cursor_field" in config:
                metadata["cursor_field"] = config["cursor_field"]
            return metadata

        def read_table(
            self, table_name: str, start_offset: dict,
            table_options: dict[str, str],
        ) -> tuple[Iterator[dict], dict]:
            if table_name not in TABLE_SCHEMAS:
                raise ValueError(
                    f"Unsupported table: {table_name}. "
                    f"Supported tables: {SUPPORTED_TABLES}"
                )

            table_readers = {
                "teams": self._read_teams,
                "channels": self._read_channels,
                "messages": self._read_messages,
                "members": self._read_members,
                "message_replies": self._read_message_replies,
            }

            reader = table_readers.get(table_name)
            if reader is None:
                raise ValueError(
                    f"No reader for table: {table_name}"
                )
            return reader(start_offset, table_options)

        # ================================================================
        # Table-Specific Read Methods
        # ================================================================

        def _read_teams(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read teams table (snapshot mode)."""
            top = parse_int_option(table_options, "top", 50)
            top = max(1, min(top, 999))
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )

            endpoint = TABLE_METADATA["teams"]["endpoint"]
            url = f"{self._client.base_url}/{endpoint}"
            params = {"$top": top}

            records: list[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url

            while next_url and pages_fetched < max_pages:
                if pages_fetched == 0:
                    data = self._client.make_request(
                        url, params=params,
                    )
                else:
                    data = self._client.make_request(next_url)

                for team in data.get("value", []):
                    record: dict[str, Any] = dict(team)
                    serialize_complex_fields(
                        record, _TEAM_SETTINGS_FIELDS,
                    )
                    records.append(record)

                next_url = data.get("@odata.nextLink")
                pages_fetched += 1
                if next_url:
                    time.sleep(0.1)

            return iter(records), {}

        def _read_channels(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read channels table (snapshot mode)."""
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            team_ids = resolve_team_ids(
                self._client, table_options, "channels", max_pages,
            )

            records: list[dict[str, Any]] = []

            for current_team_id in team_ids:
                base = self._client.base_url
                url = f"{base}/teams/{current_team_id}/channels"
                pages_fetched = 0
                next_url: str | None = url

                while next_url and pages_fetched < max_pages:
                    try:
                        if pages_fetched == 0:
                            data = self._client.make_request(
                                url, params={},
                            )
                        else:
                            data = self._client.make_request(
                                next_url,
                            )

                        for channel in data.get("value", []):
                            record: dict[str, Any] = dict(channel)
                            record["team_id"] = current_team_id
                            records.append(record)

                        next_url = data.get("@odata.nextLink")
                        pages_fetched += 1
                        if next_url:
                            time.sleep(0.1)

                    except Exception as e:
                        if ("404" not in str(e)
                                and "403" not in str(e)):
                            raise
                        break

            return iter(records), {}

        def _read_members(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read members table (snapshot mode)."""
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            team_ids = resolve_team_ids(
                self._client, table_options, "members", max_pages,
            )

            records: list[dict[str, Any]] = []

            for current_team_id in team_ids:
                base = self._client.base_url
                url = f"{base}/teams/{current_team_id}/members"
                pages_fetched = 0
                next_url: str | None = url

                while next_url and pages_fetched < max_pages:
                    try:
                        if pages_fetched == 0:
                            data = self._client.make_request(
                                url, params={},
                            )
                        else:
                            data = self._client.make_request(
                                next_url,
                            )

                        for member in data.get("value", []):
                            record: dict[str, Any] = dict(member)
                            record["team_id"] = current_team_id
                            records.append(record)

                        next_url = data.get("@odata.nextLink")
                        pages_fetched += 1
                        if next_url:
                            time.sleep(0.1)

                    except Exception as e:
                        if ("404" not in str(e)
                                and "403" not in str(e)):
                            raise
                        break

            return iter(records), {}

        def _read_messages(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read messages - routes to Delta API or legacy."""
            use_delta = (
                table_options.get("use_delta_api", "true").lower()
                == "true"
            )
            if use_delta:
                return self._read_messages_delta(
                    start_offset, table_options,
                )
            return self._read_messages_legacy(
                start_offset, table_options,
            )

        def _read_messages_delta(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read messages using Microsoft Graph Delta API."""
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            pairs = resolve_team_channel_pairs(
                self._client, table_options, "messages", max_pages,
            )

            records = []
            delta_links = {}

            for team_id, channel_id in pairs:
                try:
                    ch_recs, ch_key, ch_delta = (
                        self._fetch_channel_delta(
                            team_id, channel_id,
                            start_offset, max_pages,
                        )
                    )
                    records.extend(ch_recs)
                    if ch_delta:
                        delta_links[ch_key] = ch_delta
                except Exception as e:
                    if ("404" not in str(e)
                            and "403" not in str(e)):
                        raise

            next_offset = (
                {"deltaLinks": delta_links} if delta_links else {}
            )
            return iter(records), next_offset

        def _fetch_channel_delta(
            self, team_id, channel_id, start_offset, max_pages,
        ):
            """Fetch delta messages for a single channel."""
            channel_key = f"{team_id}/{channel_id}"

            delta_link = None
            if start_offset and isinstance(start_offset, dict):
                delta_link = start_offset.get(
                    "deltaLinks", {},
                ).get(channel_key)

            base = self._client.base_url
            url = delta_link or (
                f"{base}/teams/{team_id}"
                f"/channels/{channel_id}/messages/delta"
            )

            records = []
            new_delta_link = None
            pages_fetched = 0

            while url and pages_fetched < max_pages:
                data = self._client.make_request(url)

                for msg in data.get("value", []):
                    if "@removed" in msg:
                        record = _build_deleted_record(
                            msg["id"], team_id, channel_id,
                        )
                    else:
                        record = dict(msg)
                        record["team_id"] = team_id
                        record["channel_id"] = channel_id
                        serialize_complex_fields(
                            record, _MESSAGE_COMPLEX_FIELDS,
                        )
                    records.append(record)

                new_link = data.get("@odata.deltaLink")
                if new_link:
                    new_delta_link = new_link
                    break

                url = data.get("@odata.nextLink")
                pages_fetched += 1
                if url:
                    time.sleep(0.1)

            return records, channel_key, new_delta_link

        def _read_messages_legacy(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read messages with client-side timestamp filtering."""
            top = parse_int_option(table_options, "top", 50)
            top = max(1, min(top, 50))
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            lookback_seconds = parse_int_option(
                table_options, "lookback_seconds", 300,
            )
            cursor = get_cursor_from_offset(
                start_offset, table_options,
            )
            pairs = resolve_team_channel_pairs(
                self._client, table_options, "messages", max_pages,
            )

            records: list[dict[str, Any]] = []
            max_modified: str | None = None
            fetch_params = {
                "cursor": cursor, "top": top,
                "max_pages": max_pages,
            }

            for team_id, channel_id in pairs:
                ch_recs, ch_max = self._fetch_channel_messages(
                    team_id, channel_id, fetch_params,
                )
                records.extend(ch_recs)
                if ch_max:
                    if not max_modified or ch_max > max_modified:
                        max_modified = ch_max

            next_cursor = compute_next_cursor(
                max_modified, cursor, lookback_seconds,
            )
            next_offset = (
                {"cursor": next_cursor} if next_cursor else {}
            )
            return iter(records), next_offset

        def _fetch_channel_messages(
            self, team_id, channel_id, fetch_params,
        ):
            """Fetch messages for a channel with cursor filtering."""
            cursor = fetch_params["cursor"]
            top = fetch_params["top"]
            max_pages = fetch_params["max_pages"]

            base = self._client.base_url
            url = (
                f"{base}/teams/{team_id}"
                f"/channels/{channel_id}/messages"
            )
            params = {"$top": top}
            records = []
            max_modified = None
            pages_fetched = 0
            next_url = url

            try:
                while next_url and pages_fetched < max_pages:
                    if pages_fetched == 0:
                        data = self._client.make_request(
                            url, params=params,
                        )
                    else:
                        data = self._client.make_request(next_url)

                    for msg in data.get("value", []):
                        modified = msg.get(
                            "lastModifiedDateTime",
                        )
                        if cursor and modified and modified < cursor:
                            continue

                        record = dict(msg)
                        record["team_id"] = team_id
                        record["channel_id"] = channel_id
                        serialize_complex_fields(
                            record, _MESSAGE_COMPLEX_FIELDS,
                        )
                        records.append(record)

                        if modified:
                            if (not max_modified
                                    or modified > max_modified):
                                max_modified = modified

                    next_url = data.get("@odata.nextLink")
                    pages_fetched += 1
                    if next_url:
                        time.sleep(0.1)

            except Exception as e:
                if "404" not in str(e) and "403" not in str(e):
                    raise

            return records, max_modified

        def _read_message_replies(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read message replies - routes to Delta or legacy."""
            use_delta = (
                table_options.get("use_delta_api", "false").lower()
                == "true"
            )
            if use_delta:
                return self._read_message_replies_delta(
                    start_offset, table_options,
                )
            return self._read_message_replies_legacy(
                start_offset, table_options,
            )

        def _read_message_replies_delta(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read message replies using Microsoft Graph Delta API."""
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            pairs = resolve_team_channel_pairs(
                self._client, table_options,
                "message_replies", max_pages,
            )
            triples = _resolve_message_triples(
                self._client, pairs, table_options, max_pages,
            )

            records: list[dict[str, Any]] = []
            delta_links = {}

            for tid, cid, mid in triples:
                try:
                    r, key, delta = self._fetch_reply_delta(
                        (tid, cid, mid), start_offset, max_pages,
                    )
                    records.extend(r)
                    if delta:
                        delta_links[key] = delta
                except Exception as e:
                    if ("404" not in str(e)
                            and "403" not in str(e)):
                        raise

            next_offset = (
                {"deltaLinks": delta_links} if delta_links else {}
            )
            return iter(records), next_offset

        def _fetch_reply_delta(
            self, message_triple, start_offset, max_pages,
        ):
            """Fetch delta replies for a single message."""
            team_id, channel_id, message_id = message_triple
            message_key = (
                f"{team_id}/{channel_id}/{message_id}"
            )

            delta_link = (
                start_offset.get("deltaLinks", {}).get(message_key)
                if start_offset
                else None
            )

            base = self._client.base_url
            url = delta_link or (
                f"{base}/teams/{team_id}/channels/"
                f"{channel_id}/messages/{message_id}"
                f"/replies/delta"
            )

            records = []
            new_delta_link = None
            pages_fetched = 0

            while url and pages_fetched < max_pages:
                data = self._client.make_request(url)

                for reply in data.get("value", []):
                    if "@removed" in reply:
                        record = _build_deleted_record(
                            reply["id"], team_id, channel_id,
                        )
                        record["parent_message_id"] = message_id
                    else:
                        record = dict(reply)
                        record["parent_message_id"] = message_id
                        record["team_id"] = team_id
                        record["channel_id"] = channel_id
                        serialize_complex_fields(
                            record, _MESSAGE_COMPLEX_FIELDS,
                        )
                    records.append(record)

                new_link = data.get("@odata.deltaLink")
                if new_link:
                    new_delta_link = new_link
                    break

                url = data.get("@odata.nextLink")
                pages_fetched += 1
                if url:
                    time.sleep(0.1)

            return records, message_key, new_delta_link

        def _read_message_replies_legacy(  # pylint: disable=too-many-locals
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read message replies with timestamp filtering."""
            top = parse_int_option(table_options, "top", 50)
            top = max(1, min(top, 50))
            max_pages = parse_int_option(
                table_options, "max_pages_per_batch", 100,
            )
            lookback_seconds = parse_int_option(
                table_options, "lookback_seconds", 300,
            )
            cursor = get_cursor_from_offset(
                start_offset, table_options,
            )
            pairs = resolve_team_channel_pairs(
                self._client, table_options,
                "message_replies", max_pages,
            )
            triples = _resolve_message_triples(
                self._client, pairs, table_options, max_pages,
            )
            max_workers = parse_int_option(
                table_options, "max_concurrent_threads", 10,
            )
            fetch_params = {
                "cursor": cursor, "top": top,
                "max_pages": max_pages,
            }

            records: list[dict[str, Any]] = []
            max_modified: str | None = None

            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {
                    ex.submit(
                        self._fetch_replies_for_message,
                        tid, cid, mid, fetch_params,
                    ): (tid, cid, mid)
                    for tid, cid, mid in triples
                }

                for future in as_completed(futures):
                    try:
                        reply_recs, reply_max = future.result()
                        records.extend(reply_recs)

                        if reply_max:
                            if (not max_modified
                                    or reply_max > max_modified):
                                max_modified = reply_max

                    except Exception as e:
                        if ("404" not in str(e)
                                and "403" not in str(e)):
                            raise

            next_cursor = compute_next_cursor(
                max_modified, cursor, lookback_seconds,
            )
            next_offset = (
                {"cursor": next_cursor} if next_cursor else {}
            )
            return iter(records), next_offset

        def _fetch_replies_for_message(
            self, team_id, channel_id, message_id, fetch_params,
        ) -> tuple[list[dict[str, Any]], str | None]:
            """Fetch replies for a single message (parallel exec)."""
            cursor = fetch_params["cursor"]
            top = fetch_params["top"]
            max_pages = fetch_params["max_pages"]

            records: list[dict[str, Any]] = []
            max_modified: str | None = None

            base = self._client.base_url
            url = (
                f"{base}/teams/{team_id}/channels/{channel_id}"
                f"/messages/{message_id}/replies"
            )
            params = {"$top": top}
            pages_fetched = 0
            next_url: str | None = url

            try:
                while next_url and pages_fetched < max_pages:
                    if pages_fetched == 0:
                        data = self._client.make_request(
                            url, params=params,
                        )
                    else:
                        data = self._client.make_request(next_url)

                    for reply in data.get("value", []):
                        modified = reply.get(
                            "lastModifiedDateTime",
                        )
                        if cursor and modified and modified < cursor:
                            continue

                        record: dict[str, Any] = dict(reply)
                        record["parent_message_id"] = message_id
                        record["team_id"] = team_id
                        record["channel_id"] = channel_id
                        serialize_complex_fields(
                            record, _MESSAGE_COMPLEX_FIELDS,
                        )
                        records.append(record)

                        if modified:
                            if (not max_modified
                                    or modified > max_modified):
                                max_modified = modified

                    next_url = data.get("@odata.nextLink")
                    pages_fetched += 1
                    if next_url:
                        time.sleep(0.1)

            except Exception as e:
                if "404" not in str(e):
                    raise

            return records, max_modified


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = MicrosoftTeamsLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)  # pylint: disable=abstract-class-instantiated

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
