# ==============================================================================
# Merged Lakeflow Source: zoho_crm
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator, Optional
import json
import re
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import logging
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """

        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the records of a table and return an iterator of records and an offset.
            The read starts from the provided start_offset.
            Records returned in the iterator will be one batch of records marked by the
            offset as its end_offset.
            The read_table function could be called multiple times to read the entire table
            in multiple batches and it stops when the same offset is returned again.
            If the table cannot be incrementally read, the offset can be None if we want to
            read the entire table in one batch.
            We could still return some fake offsets (cannot checkpointing) to split the
            table into multiple batches.
            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                An iterator of records in JSON format and an offset.
                DO NOT convert the JSON based on the schema in `get_table_schema` in
                `read_table`.
                records: An iterator of records in JSON format.
                offset: An offset in dict.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                An iterator of deleted records in JSON format and an offset.
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: An offset in dict (same format as read_table).
            """


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/zoho_client.py
    ########################################################

    class ZohoAPIError(Exception):
        """Exception for Zoho CRM API errors."""

        # Friendly messages for common Zoho error codes
        # See: https://www.zoho.com/developer/help/api/error-messages.html
        KNOWN_ERRORS = {
            # OAuth/Auth errors
            "INVALID_TOKEN": "Access token expired or invalid.",
            "INVALID_CLIENT": "Invalid client_id or client_secret.",
            "AUTHENTICATION_FAILURE": "Authentication failed. Check OAuth credentials.",
            "NO_PERMISSION": "Missing required OAuth scopes.",
            "OAUTH_SCOPE_MISMATCH": "OAuth scope mismatch. Re-authorize with correct scopes.",
            # Zoho API error codes
            "4000": "Use OAuth token instead of API ticket.",
            "4001": "No API permission for this operation.",
            "4101": "Zoho CRM is disabled for this account.",
            "4102": "No CRM account found.",
            "4103": "Record not found with the specified ID.",
            "4401": "Mandatory field missing in request.",
            "4420": "Invalid search parameter or value.",
            "4421": "API call limit exceeded.",
            "4422": "No records available in this module.",
            "4423": "Exceeded record search limit.",
            "4500": "Internal server error.",
            "4501": "API Key is inactive.",
            "4502": "This module is not supported in your Zoho CRM edition.",
            "4600": "Invalid API parameter or spelling error in API URL.",
            "4807": "File size limit exceeded.",
            "4809": "Storage space limit exceeded.",
            "4820": "Rate limit exceeded. Wait before retrying.",
            "4831": "Missing required parameters.",
            "4832": "Invalid data type (text given for integer field).",
            "4834": "Invalid or expired ticket.",
            "4890": "Wrong API Key.",
            # Permission errors
            "401": "No module permission.",
            "401.1": "No permission to create records.",
            "401.2": "No permission to edit records.",
            "401.3": "No permission to delete records.",
            # Module errors
            "INVALID_MODULE": "Module not available in your Zoho CRM edition.",
            "MODULE_NOT_SUPPORTED": "This module is not accessible via API.",
        }

        def __init__(self, status_code: int, message: str):
            self.status_code = status_code
            super().__init__(f"Zoho API error ({status_code}): {message}")

        @classmethod
        def from_response(cls, response: requests.Response) -> "ZohoAPIError":
            """Create exception from HTTP response."""
            error_code = None
            message = response.text[:200]

            # Try to parse Zoho's JSON error response
            try:
                data = response.json()
                error_code = data.get("code") or data.get("error")
                message = data.get("message") or data.get("error_description") or message
            except ValueError:
                pass

            # Use friendly message if we recognize the error code
            if error_code and error_code in cls.KNOWN_ERRORS:
                message = cls.KNOWN_ERRORS[error_code]

            return cls(response.status_code, message)


    class ZohoAPIClient:  # pylint: disable=too-many-instance-attributes
        """
        HTTP client for Zoho CRM API with OAuth2 authentication.

        Handles:
        - OAuth2 token refresh
        - Rate limiting with exponential backoff
        - Paginated API responses
        """

        def __init__(
            self,
            client_id: str,
            client_secret: str,
            refresh_token: str,
            accounts_url: str = "https://accounts.zoho.com",
        ) -> None:
            """
            Initialize the API client.

            Args:
                client_id: OAuth Client ID from Zoho API Console
                client_secret: OAuth Client Secret from Zoho API Console
                refresh_token: Long-lived refresh token from OAuth flow
                accounts_url: Zoho accounts URL for OAuth (region-specific)
            """
            self.client_id = client_id
            self.client_secret = client_secret
            self.refresh_token = refresh_token
            self.accounts_url = accounts_url.rstrip("/")

            # Derive API URL from accounts URL
            # https://accounts.zoho.eu -> https://www.zohoapis.eu
            match = re.search(r"accounts\.zoho\.(.+)$", self.accounts_url)
            domain_suffix = match.group(1) if match else "com"
            self.api_url = f"https://www.zohoapis.{domain_suffix}"

            # Token management
            self._access_token: Optional[str] = None
            self._token_expires_at: Optional[datetime] = None

            # HTTP session for connection pooling
            self._session = requests.Session()

        def _get_access_token(self) -> str:
            """
            Get a valid access token, refreshing if necessary.
            Access tokens expire after 1 hour (3600 seconds).
            """
            # Check if we have a valid token (with 5-minute buffer)
            if self._access_token and self._token_expires_at:
                if datetime.now() < self._token_expires_at - timedelta(minutes=5):
                    return self._access_token

            # Refresh the token
            token_url = f"{self.accounts_url}/oauth/v2/token"
            data = {
                "refresh_token": self.refresh_token,
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "grant_type": "refresh_token",
            }

            response = requests.post(token_url, data=data, timeout=30)

            if response.status_code >= 400:
                raise ZohoAPIError.from_response(response)

            token_data = response.json()

            if "access_token" not in token_data:
                raise ZohoAPIError(200, "Token refresh failed. Check your OAuth credentials.")

            self._access_token = token_data["access_token"]
            expires_in = token_data.get("expires_in", 3600)
            self._token_expires_at = datetime.now() + timedelta(seconds=expires_in)

            return self._access_token

        def request(  # pylint: disable=too-many-arguments,too-many-positional-arguments
            self,
            method: str,
            endpoint: str,
            params: Optional[dict] = None,
            data: Optional[dict] = None,
            max_retries: int = 3,
        ) -> dict:
            """
            Make an authenticated API request to Zoho CRM.

            Args:
                method: HTTP method (GET, POST, PUT, DELETE)
                endpoint: API endpoint path (e.g., "/crm/v8/Leads")
                params: Query parameters
                data: Request body for POST/PUT
                max_retries: Maximum retry attempts for rate limiting

            Returns:
                Parsed JSON response as dictionary

            Raises:
                Exception: On API errors after retries exhausted
            """
            access_token = self._get_access_token()
            url = f"{self.api_url}{endpoint}"
            headers = {"Authorization": f"Zoho-oauthtoken {access_token}"}

            if data:
                headers["Content-Type"] = "application/json"

            for attempt in range(max_retries):
                response = self._make_http_request(method, url, headers, params, data)

                # Handle rate limiting with retry
                if response.status_code == 429:
                    if attempt < max_retries - 1:
                        wait_time = 2**attempt
                        time.sleep(wait_time)
                        continue
                    raise ZohoAPIError.from_response(response)

                # Handle 401 with token refresh retry
                if response.status_code == 401 and attempt == 0:
                    self._access_token = None
                    access_token = self._get_access_token()
                    headers["Authorization"] = f"Zoho-oauthtoken {access_token}"
                    continue

                # Handle other errors
                if response.status_code >= 400:
                    raise ZohoAPIError.from_response(response)

                # Handle empty responses
                if not response.text or response.text.strip() == "":
                    return {}

                return response.json()

            raise ZohoAPIError(0, f"Failed after {max_retries} retries")

        def _make_http_request(  # pylint: disable=too-many-arguments,too-many-positional-arguments
            self,
            method: str,
            url: str,
            headers: dict,
            params: Optional[dict],
            data: Optional[dict],
        ) -> requests.Response:
            """
            Execute the actual HTTP request.

            Args:
                method: HTTP method (GET, POST, PUT, DELETE)
                url: Full URL to request
                headers: Request headers including Authorization
                params: Query parameters
                data: Request body for POST/PUT

            Returns:
                requests.Response object

            Raises:
                ValueError: For unsupported HTTP methods
            """
            method = method.upper()
            if method == "GET":
                return self._session.get(url, headers=headers, params=params)
            elif method == "POST":
                return self._session.post(url, headers=headers, json=data, params=params)
            elif method == "PUT":
                return self._session.put(url, headers=headers, json=data, params=params)
            elif method == "DELETE":
                return self._session.delete(url, headers=headers, params=params)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

        def paginate(
            self,
            endpoint: str,
            params: Optional[dict] = None,
            data_key: str = "data",
            per_page: int = 200,
        ) -> Iterator[dict]:
            """
            Iterate through paginated API responses.

            Args:
                endpoint: API endpoint path
                params: Base query parameters (page/per_page will be added)
                data_key: Key in response containing the data array
                per_page: Number of records per page (max 200 for Zoho)

            Yields:
                Individual records from each page
            """
            params = dict(params) if params else {}
            page = 1

            while True:
                params["page"] = page
                params["per_page"] = per_page

                response = self.request("GET", endpoint, params=params)
                data = response.get(data_key, [])
                info = response.get("info", {})

                yield from data

                if not info.get("more_records", False) or not data:
                    break

                page += 1

        def paginate_with_info(
            self,
            endpoint: str,
            params: Optional[dict] = None,
            data_key: str = "data",
            per_page: int = 200,
        ) -> Iterator[tuple[list[dict], dict]]:
            """
            Iterate through paginated API responses, yielding page data with info.

            Useful when you need access to pagination metadata.

            Args:
                endpoint: API endpoint path
                params: Base query parameters
                data_key: Key in response containing the data array
                per_page: Number of records per page

            Yields:
                Tuples of (records_list, info_dict) for each page
            """
            params = dict(params) if params else {}
            page = 1

            while True:
                params["page"] = page
                params["per_page"] = per_page

                response = self.request("GET", endpoint, params=params)
                data = response.get(data_key, [])
                info = response.get("info", {})

                yield data, info

                if not info.get("more_records", False) or not data:
                    break

                page += 1


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/handlers/base.py
    ########################################################

    class TableHandler(ABC):
        """
        Abstract base class for handling different types of Zoho CRM tables.

        Each handler is responsible for:
        - Returning the table schema
        - Returning table metadata (primary keys, ingestion type, etc.)
        - Reading records from the table
        """

        def __init__(self, client: ZohoAPIClient) -> None:
            """
            Initialize the handler with an API client.

            Args:
                client: ZohoAPIClient instance for making API requests
            """
            self.client = client

        @abstractmethod
        def get_schema(self, table_name: str, config: dict) -> StructType:
            """
            Get the Spark schema for a table.

            Args:
                table_name: Name of the table
                config: Table configuration dictionary

            Returns:
                Spark StructType representing the table schema
            """

        @abstractmethod
        def get_metadata(self, table_name: str, config: dict) -> dict:
            """
            Get metadata for a table.

            Args:
                table_name: Name of the table
                config: Table configuration dictionary

            Returns:
                Dictionary with keys:
                    - primary_keys: List of primary key column names
                    - cursor_field: (optional) Field name for incremental loading
                    - ingestion_type: "snapshot", "cdc", "cdc_with_deletes", or "append"
            """

        @abstractmethod
        def read(
            self,
            table_name: str,
            config: dict,
            start_offset: dict,
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a table.

            Args:
                table_name: Name of the table
                config: Table configuration dictionary
                start_offset: Offset to start reading from

            Returns:
                Tuple of (records_iterator, next_offset)
            """


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/zoho_types.py
    ########################################################

    SIMPLE_TYPE_MAP = {
        "bigint": LongType(),
        "text": StringType(),
        "textarea": StringType(),
        "email": StringType(),
        "phone": StringType(),
        "website": StringType(),
        "autonumber": StringType(),
        "picklist": StringType(),
        "integer": LongType(),
        "double": DoubleType(),
        "currency": DoubleType(),
        "percent": DoubleType(),
        "boolean": BooleanType(),
        "date": StringType(),  # Stored as ISO 8601 string
        "datetime": StringType(),  # Stored as ISO 8601 string
        "fileupload": StringType(),
        "imageupload": StringType(),
        "profileimage": StringType(),
        "event_reminder": StringType(),
    }


    # =============================================================================
    # Reusable Schema Components
    # =============================================================================

    # Basic lookup field structure (id + name)
    BASIC_LOOKUP_SCHEMA = StructType([
        StructField("id", StringType(), True),
        StructField("name", StringType(), True),
    ])

    # Extended lookup field structure (id + name + email)
    EXTENDED_LOOKUP_SCHEMA = StructType([
        StructField("id", StringType(), True),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
    ])

    # RRULE field for recurring events
    RRULE_SCHEMA = StructType([
        StructField("FREQ", StringType(), True),
        StructField("INTERVAL", StringType(), True),
    ])

    # ALARM field for event reminders
    ALARM_SCHEMA = StructType([
        StructField("ACTION", StringType(), True),
    ])

    # Basic subform item structure
    BASIC_SUBFORM_SCHEMA = StructType([
        StructField("id", StringType(), True),
    ])


    # =============================================================================
    # Settings Table Schemas
    # =============================================================================

    USERS_SCHEMA = StructType([
        StructField("id", StringType(), False),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
        StructField("first_name", StringType(), True),
        StructField("last_name", StringType(), True),
        StructField("role", BASIC_LOOKUP_SCHEMA, True),
        StructField("profile", BASIC_LOOKUP_SCHEMA, True),
        StructField("status", StringType(), True),
        StructField("created_time", StringType(), True),
        StructField("Modified_Time", StringType(), True),
        StructField("confirm", BooleanType(), True),
        StructField("territories", ArrayType(BASIC_LOOKUP_SCHEMA), True),
    ])

    ROLES_SCHEMA = StructType([
        StructField("id", StringType(), False),
        StructField("name", StringType(), True),
        StructField("display_label", StringType(), True),
        StructField("reporting_to", BASIC_LOOKUP_SCHEMA, True),
        StructField("admin_user", BooleanType(), True),
    ])

    PROFILES_SCHEMA = StructType([
        StructField("id", StringType(), False),
        StructField("name", StringType(), True),
        StructField("display_label", StringType(), True),
        StructField("default", BooleanType(), True),
        StructField("description", StringType(), True),
        StructField("created_time", StringType(), True),
        StructField("Modified_Time", StringType(), True),
    ])

    SETTINGS_SCHEMAS = {
        "Users": USERS_SCHEMA,
        "Roles": ROLES_SCHEMA,
        "Profiles": PROFILES_SCHEMA,
    }


    # =============================================================================
    # Subform/Line Item Schema
    # =============================================================================

    SUBFORM_BASE_FIELDS = [
        StructField("id", StringType(), False),
        StructField("_parent_id", StringType(), False),
        StructField("_parent_module", StringType(), False),
    ]

    SUBFORM_COMMON_FIELDS = [
        StructField("Product_Name", BASIC_LOOKUP_SCHEMA, True),
        StructField("Quantity", DoubleType(), True),
        StructField("Unit_Price", DoubleType(), True),
        StructField("List_Price", DoubleType(), True),
        StructField("Net_Total", DoubleType(), True),
        StructField("Total", DoubleType(), True),
        StructField("Discount", DoubleType(), True),
        StructField("Total_After_Discount", DoubleType(), True),
        StructField("Tax", DoubleType(), True),
        StructField("Description", StringType(), True),
        StructField("Sequence_Number", LongType(), True),
    ]

    LINE_ITEM_SCHEMA = StructType(SUBFORM_BASE_FIELDS + SUBFORM_COMMON_FIELDS)


    # =============================================================================
    # Junction/Related Table Schemas
    # =============================================================================

    JUNCTION_BASE_FIELDS = [
        StructField("_junction_id", StringType(), False),
        StructField("_parent_id", StringType(), False),
        StructField("_parent_module", StringType(), False),
        StructField("id", StringType(), False),
    ]

    LEADS_RELATED_FIELDS = [
        StructField("First_Name", StringType(), True),
        StructField("Last_Name", StringType(), True),
        StructField("Email", StringType(), True),
        StructField("Company", StringType(), True),
        StructField("Phone", StringType(), True),
        StructField("Lead_Status", StringType(), True),
    ]

    CONTACTS_RELATED_FIELDS = [
        StructField("First_Name", StringType(), True),
        StructField("Last_Name", StringType(), True),
        StructField("Email", StringType(), True),
        StructField("Phone", StringType(), True),
        StructField("Account_Name", BASIC_LOOKUP_SCHEMA, True),
    ]

    CONTACT_ROLES_RELATED_FIELDS = [
        StructField("Contact_Role", StringType(), True),
        StructField("name", StringType(), True),
        StructField("Email", StringType(), True),
    ]

    DEFAULT_RELATED_FIELDS = [
        StructField("name", StringType(), True),
    ]

    # Map related module names to their field schemas
    RELATED_MODULE_FIELDS = {
        "Leads": LEADS_RELATED_FIELDS,
        "Contacts": CONTACTS_RELATED_FIELDS,
        "Contact_Roles": CONTACT_ROLES_RELATED_FIELDS,
    }

    # API field names for related record queries
    RELATED_MODULE_API_FIELDS = {
        "Leads": "id,First_Name,Last_Name,Email,Company,Phone,Lead_Status",
        "Contacts": "id,First_Name,Last_Name,Email,Phone,Account_Name",
        "Deals": "id,Deal_Name,Stage,Amount,Closing_Date,Account_Name",
        "Contact_Roles": "id,Contact_Role,name,Email",
    }


    # =============================================================================
    # Type Conversion Functions
    # =============================================================================

    def zoho_field_to_spark_type(field: dict) -> StructField:
        """
        Convert a Zoho CRM field definition to a Spark StructField.

        Args:
            field: Zoho field metadata dictionary containing api_name, data_type, etc.

        Returns:
            A Spark StructField representing the field.
        """
        api_name = field["api_name"]
        data_type = field.get("data_type", "text")
        json_type = field.get("json_type")
        nullable = not field.get("required", False)

        # Check simple type map first
        if data_type in SIMPLE_TYPE_MAP:
            spark_type = SIMPLE_TYPE_MAP[data_type]
        # Handle complex/nested types
        elif data_type == "multiselectpicklist":
            spark_type = ArrayType(StringType(), True)
        elif data_type in ("lookup", "ownerlookup"):
            spark_type = EXTENDED_LOOKUP_SCHEMA
        elif data_type == "multiselectlookup":
            spark_type = ArrayType(BASIC_LOOKUP_SCHEMA, True)
        elif data_type == "subform":
            spark_type = ArrayType(BASIC_SUBFORM_SCHEMA, True)
        elif data_type == "consent_lookup":
            spark_type = BASIC_LOOKUP_SCHEMA
        elif data_type == "RRULE":
            spark_type = RRULE_SCHEMA
        elif data_type == "ALARM":
            spark_type = ALARM_SCHEMA
        else:
            # Default to StringType for unknown types
            spark_type = StringType()

        # Handle special JSON types - store as JSON strings for flexibility
        if json_type in ("jsonarray", "jsonobject"):
            spark_type = StringType()

        return StructField(api_name, spark_type, nullable)


    def get_related_table_schema(related_module: str) -> StructType:
        """
        Build schema for a junction/related table.

        Args:
            related_module: Name of the related module (e.g., "Leads", "Contacts")

        Returns:
            A StructType representing the junction table schema.
        """
        related_fields = RELATED_MODULE_FIELDS.get(related_module, DEFAULT_RELATED_FIELDS)
        return StructType(JUNCTION_BASE_FIELDS + related_fields)


    def normalize_record(record: dict, json_fields: set) -> dict:
        """
        Normalize a record for Spark compatibility.
        Only serializes fields that are declared as JSON strings in schema.

        Args:
            record: Raw record from Zoho API
            json_fields: Set of field names that should be JSON-serialized

        Returns:
            Normalized record dictionary
        """
        normalized = {}
        for key, value in record.items():
            if value is None:
                normalized[key] = None
            elif key in json_fields and isinstance(value, (dict, list)):
                normalized[key] = json.dumps(value)
            else:
                normalized[key] = value
        return normalized


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/handlers/module.py
    ########################################################

    logger = logging.getLogger(__name__)


    class ModuleHandler(TableHandler):
        """
        Handler for standard Zoho CRM modules.

        Standard modules:
        - Support the Records API (/crm/v8/{module})
        - Have a Modified_Time field for CDC
        - Support the Deleted Records API for tracking deletions
        """

        # Modules to exclude from listing
        EXCLUDED_MODULES = {
            "Visits",  # No fields available
            "Actions_Performed",  # No fields available
            "Email_Sentiment",  # Analytics module with different API
            "Email_Analytics",  # Analytics module with different API
            "Email_Template_Analytics",  # Analytics module with different API
            "Locking_Information__s",  # System module (403 forbidden)
        }

        def __init__(self, client) -> None:
            super().__init__(client)
            self._modules_cache: Optional[list[dict]] = None
            self._fields_cache: dict[str, list[dict]] = {}

        def get_modules(self) -> list[dict]:
            """
            Retrieve all available modules from Zoho CRM.
            Results are cached to avoid repeated API calls.
            """
            if self._modules_cache is not None:
                return self._modules_cache

            response = self.client.request("GET", "/crm/v8/settings/modules")
            modules = response.get("modules", [])

            # Filter for API-supported modules
            supported = [
                m
                for m in modules
                if m.get("api_supported")
                and m.get("generated_type") in ("default", "custom")
                and m.get("api_name") not in self.EXCLUDED_MODULES
            ]

            self._modules_cache = supported
            return supported

        def get_fields(self, module_name: str) -> list[dict]:
            """
            Retrieve field metadata for a specific module.
            Results are cached per module.
            """
            if module_name in self._fields_cache:
                return self._fields_cache[module_name]

            response = self.client.request(
                "GET",
                "/crm/v8/settings/fields",
                params={"module": module_name},
            )
            fields = response.get("fields", [])

            self._fields_cache[module_name] = fields
            return fields

        def get_json_fields(self, module_name: str) -> set:
            """
            Get field names that should be serialized as JSON strings.

            Args:
                module_name: Name of the Zoho CRM module

            Returns:
                Set of field API names with json_type 'jsonobject' or 'jsonarray'
            """
            fields = self.get_fields(module_name)
            return {
                f.get("api_name") for f in fields if f.get("json_type") in ("jsonobject", "jsonarray")
            }

        def get_schema(self, table_name: str, config: dict) -> StructType:
            """
            Get Spark schema for a standard CRM module.

            Dynamically builds the schema by fetching field metadata from the
            Zoho CRM Fields API and converting each field to a Spark StructField.

            Args:
                table_name: Name of the Zoho CRM module
                config: Table configuration (unused for standard modules)

            Returns:
                Spark StructType representing the module schema
            """
            fields = self.get_fields(table_name)

            if not fields:
                logger.warning("No fields available for %s, using minimal schema", table_name)
                return StructType([StructField("id", LongType(), False)])

            struct_fields = []
            for field in fields:
                try:
                    struct_fields.append(zoho_field_to_spark_type(field))
                except Exception as e:
                    logger.warning("Could not convert field %s: %s", field.get("api_name"), e)
                    continue

            return StructType(struct_fields)

        def get_metadata(self, table_name: str, config: dict) -> dict:
            """
            Get ingestion metadata for a standard CRM module.

            Determines the appropriate ingestion strategy based on available fields:
            - CDC for modules with Modified_Time field
            - Append for Attachments
            - Snapshot for modules without Modified_Time

            Args:
                table_name: Name of the Zoho CRM module
                config: Table configuration (unused for standard modules)

            Returns:
                Dictionary with primary_keys, cursor_field, and ingestion_type
            """
            schema = self.get_schema(table_name, config)
            field_names = schema.fieldNames()
            has_modified_time = "Modified_Time" in field_names
            has_id = "id" in field_names

            # Attachments are append-only
            if table_name == "Attachments":
                return {
                    "primary_keys": ["id"] if has_id else [],
                    "ingestion_type": "append",
                }

            # Modules without Modified_Time use snapshot
            if not has_modified_time:
                return {
                    "primary_keys": ["id"] if has_id else [],
                    "ingestion_type": "snapshot",
                }

            # Standard modules support CDC
            return {
                "primary_keys": ["id"],
                "cursor_field": "Modified_Time",
                "ingestion_type": "cdc",
            }

        def read(
            self,
            table_name: str,
            config: dict,
            start_offset: dict,
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a standard CRM module.

            Supports incremental reads using Modified_Time cursor with a 5-minute
            lookback window to catch late updates. For CDC modules, also fetches
            deleted records via the Deleted Records API.

            Args:
                table_name: Name of the Zoho CRM module
                config: Table configuration with optional initial_load_start_date
                start_offset: Dictionary with cursor_time for incremental reads

            Returns:
                Tuple of (records iterator, next offset dictionary)
            """
            # Determine cursor time for incremental reads
            cursor_time = start_offset.get("cursor_time") if start_offset else None
            initial_load_start_date = config.get("initial_load_start_date")

            if not cursor_time and initial_load_start_date:
                cursor_time = initial_load_start_date

            # Apply 5-minute lookback window to catch late updates
            if cursor_time:
                cursor_dt = datetime.fromisoformat(cursor_time.replace("Z", "+00:00"))
                lookback_dt = cursor_dt - timedelta(minutes=5)
                cursor_time = lookback_dt.strftime("%Y-%m-%dT%H:%M:%S+00:00")

            # Check if this is a snapshot table (no cursor support)
            metadata = self.get_metadata(table_name, config)
            if metadata.get("ingestion_type") == "snapshot":
                cursor_time = None

            # Track max modified time for next offset
            max_modified_time = cursor_time

            def records_generator():
                nonlocal max_modified_time
                json_fields = self.get_json_fields(table_name)
                fields = self.get_fields(table_name)
                field_names = [f["api_name"] for f in fields] if fields else []

                # Read regular records
                for record in self._read_records(table_name, field_names, cursor_time, json_fields):
                    modified_time = record.get("Modified_Time")
                    if modified_time and (not max_modified_time or modified_time > max_modified_time):
                        max_modified_time = modified_time
                    yield record

                # Read deleted records for CDC
                if metadata.get("ingestion_type") == "cdc" and cursor_time:
                    for record in self._read_deleted_records(table_name, cursor_time):
                        deleted_time = record.get("deleted_time")
                        if deleted_time and (not max_modified_time or deleted_time > max_modified_time):
                            max_modified_time = deleted_time
                        yield record

            # Materialize generator to get final max_modified_time
            records = list(records_generator())

            if max_modified_time:
                next_offset = {"cursor_time": max_modified_time}
            else:
                next_offset = start_offset or {}
            return iter(records), next_offset

        def _read_records(
            self,
            module_name: str,
            field_names: list[str],
            cursor_time: Optional[str],
            json_fields: set,
        ) -> Iterator[dict]:
            """Read records from a module with pagination."""
            params = {
                "sort_order": "asc",
                "sort_by": "Modified_Time",
            }

            if field_names:
                params["fields"] = ",".join(field_names)

            if cursor_time:
                params["criteria"] = f"(Modified_Time:greater_equal:{cursor_time})"

            for record in self.client.paginate(f"/crm/v8/{module_name}", params=params):
                yield normalize_record(record, json_fields)

        def _read_deleted_records(
            self,
            module_name: str,
            cursor_time: str,
        ) -> Iterator[dict]:
            """Read deleted records from a module."""
            params = {"type": "all"}

            for record in self.client.paginate(f"/crm/v8/{module_name}/deleted", params=params):
                deleted_time = record.get("deleted_time")

                # Only include records deleted after cursor_time
                if deleted_time and deleted_time >= cursor_time:
                    record["_zoho_deleted"] = True
                    yield record


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/handlers/related.py
    ########################################################

    logger = logging.getLogger(__name__)


    # Configuration for related/junction tables
    RELATED_TABLES = {
        "Campaigns_Leads": {
            "parent_module": "Campaigns",
            "related_module": "Leads",
        },
        "Campaigns_Contacts": {
            "parent_module": "Campaigns",
            "related_module": "Contacts",
        },
        "Contacts_X_Deals": {
            "parent_module": "Deals",
            "related_module": "Contact_Roles",
        },
    }


    class RelatedHandler(TableHandler):
        """
        Handler for Zoho CRM junction/related record tables.

        Junction tables represent many-to-many relationships by fetching
        related records for each parent record via the Related Records API.

        These tables:
        - Use the Related Records API (/crm/v8/{parent}/{id}/{related})
        - Include junction metadata (_junction_id, _parent_id, _parent_module)
        - Use snapshot ingestion (relationships can change without timestamp)
        """

        @staticmethod
        def get_tables() -> dict[str, dict]:
            """Return configuration for all related tables."""
            return RELATED_TABLES

        def get_schema(self, table_name: str, config: dict) -> StructType:
            """
            Get Spark schema for a junction table.

            Junction table schemas include standard junction metadata fields
            (_junction_id, _parent_id, _parent_module) plus related module fields.

            Args:
                table_name: Name of the junction table
                config: Table configuration (unused for junction tables)

            Returns:
                Spark StructType representing the junction table schema
            """
            table_config = RELATED_TABLES.get(table_name, {})
            related_module = table_config.get("related_module", "")
            return get_related_table_schema(related_module)

        def get_metadata(self, table_name: str, config: dict) -> dict:
            """
            Get ingestion metadata for a junction table.

            Junction tables use snapshot ingestion with a composite key since
            relationships can be created/deleted without timestamps.

            Args:
                table_name: Name of the junction table
                config: Table configuration (unused for junction tables)

            Returns:
                Dictionary with primary_keys=['_junction_id'] and ingestion_type='snapshot'
            """
            # Junction tables use snapshot with composite key
            return {
                "primary_keys": ["_junction_id"],
                "ingestion_type": "snapshot",
            }

        def read(
            self,
            table_name: str,
            config: dict,
            start_offset: dict,
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a junction table.

            Iterates through all parent records and fetches their related records
            using the Zoho CRM Related Records API. Each junction record includes
            the related record data plus metadata (_junction_id, _parent_id, _parent_module).

            Args:
                table_name: Name of the junction table
                config: Table configuration with parent_module and related_module
                start_offset: Offset dictionary (unused - junction tables use snapshot)

            Returns:
                Tuple of (records iterator, empty offset dict)
            """
            table_config = RELATED_TABLES.get(table_name, {})
            parent_module = table_config.get("parent_module", "")
            related_module = table_config.get("related_module", "")

            # Get API fields for related module
            related_fields = RELATED_MODULE_API_FIELDS.get(related_module, "id,name")

            def records_generator():
                # First, collect all parent IDs
                parent_ids = list(self._get_parent_ids(parent_module))

                # Fetch related records for each parent
                for parent_id in parent_ids:
                    related_records = self._get_related_records(
                        parent_module, parent_id, related_module, related_fields
                    )
                    for record in related_records:
                        record["_junction_id"] = f"{parent_id}_{record.get('id')}"
                        record["_parent_id"] = parent_id
                        record["_parent_module"] = parent_module
                        yield record

            # Junction tables use snapshot - no cursor tracking
            return records_generator(), {}

        def _get_parent_ids(self, parent_module: str) -> Iterator[str]:
            """
            Get all record IDs from a parent module.

            Args:
                parent_module: Name of the parent Zoho CRM module

            Yields:
                Record IDs as strings
            """
            params = {"fields": "id"}
            for record in self.client.paginate(f"/crm/v8/{parent_module}", params=params):
                if record.get("id"):
                    yield record["id"]

        def _get_related_records(
            self,
            parent_module: str,
            parent_id: str,
            related_module: str,
            fields: str,
        ) -> Iterator[dict]:
            """
            Get related records for a specific parent record.

            Uses the Zoho CRM Related Records API to fetch records linked
            to a parent record via a many-to-many relationship.

            Args:
                parent_module: Name of the parent module (e.g., "Campaigns")
                parent_id: ID of the parent record
                related_module: Name of the related module (e.g., "Leads")
                fields: Comma-separated field names to retrieve

            Yields:
                Related record dictionaries
            """
            endpoint = f"/crm/v8/{parent_module}/{parent_id}/{related_module}"
            params = {"fields": fields}

            try:
                yield from self.client.paginate(endpoint, params=params)
            except ZohoAPIError as e:
                # 204/400/404 means no related records - not an error
                if e.status_code in (204, 400, 404):
                    return
                raise


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/handlers/settings.py
    ########################################################

    logger = logging.getLogger(__name__)


    # Configuration for settings tables
    SETTINGS_TABLES = {
        "Users": {
            "endpoint": "/crm/v8/users",
            "data_key": "users",
            "supports_cdc": True,
        },
        "Roles": {
            "endpoint": "/crm/v8/settings/roles",
            "data_key": "roles",
            "supports_cdc": False,
        },
        "Profiles": {
            "endpoint": "/crm/v8/settings/profiles",
            "data_key": "profiles",
            "supports_cdc": False,
        },
    }


    class SettingsHandler(TableHandler):
        """
        Handler for Zoho CRM settings/organization tables.

        Settings tables:
        - Users: All users in the organization (requires ZohoCRM.users.READ scope)
        - Roles: User roles hierarchy
        - Profiles: Permission profiles

        These tables use different API endpoints than standard modules.
        """

        @staticmethod
        def get_tables() -> dict[str, dict]:
            """Return configuration for all settings tables."""
            return SETTINGS_TABLES

        def get_schema(self, table_name: str, config: dict) -> StructType:
            """
            Get Spark schema for a settings table.

            Settings tables have predefined schemas since they have fixed structures.

            Args:
                table_name: Name of the settings table (Users, Roles, or Profiles)
                config: Table configuration (unused for settings)

            Returns:
                Spark StructType representing the table schema
            """
            if table_name in SETTINGS_SCHEMAS:
                return SETTINGS_SCHEMAS[table_name]

            # Fallback minimal schema
            return StructType([StructField("id", StringType(), False)])

        def get_metadata(self, table_name: str, config: dict) -> dict:
            """
            Get ingestion metadata for a settings table.

            Users supports CDC via Modified_Time. Roles and Profiles use snapshot
            since they don't have modification timestamps.

            Args:
                table_name: Name of the settings table
                config: Table configuration (unused for settings)

            Returns:
                Dictionary with primary_keys, cursor_field (if CDC), and ingestion_type
            """
            table_config = SETTINGS_TABLES.get(table_name, {})

            if table_config.get("supports_cdc"):
                return {
                    "primary_keys": ["id"],
                    "cursor_field": "Modified_Time",
                    "ingestion_type": "cdc",
                }

            return {
                "primary_keys": ["id"],
                "ingestion_type": "snapshot",
            }

        def read(
            self,
            table_name: str,
            config: dict,
            start_offset: dict,
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a settings table.

            Uses the appropriate Zoho settings API endpoint based on table type.
            Users endpoint requires the ZohoCRM.users.READ OAuth scope.

            Args:
                table_name: Name of the settings table
                config: Table configuration with endpoint and data_key
                start_offset: Offset dictionary (unused - settings use snapshot)

            Returns:
                Tuple of (records iterator, empty offset dict)
            """
            table_config = SETTINGS_TABLES.get(table_name, {})
            endpoint = table_config.get("endpoint", "")
            data_key = table_config.get("data_key", "data")

            def records_generator():
                params = {}
                if table_name == "Users":
                    params["type"] = "AllUsers"

                yield from self.client.paginate(endpoint, params=params, data_key=data_key)

            # Settings tables use snapshot - no cursor tracking
            return records_generator(), {}


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/handlers/subform.py
    ########################################################

    logger = logging.getLogger(__name__)


    # Configuration for subform tables
    # NOTE: Inventory/Order Management tables are commented out - see module docstring
    SUBFORM_TABLES: dict[str, dict] = {
        # Uncomment if you have Zoho Inventory or Zoho Books enabled:
        # "Quoted_Items": {
        #     "parent_module": "Quotes",
        #     "subform_field": "Quoted_Items",
        # },
        # "Ordered_Items": {
        #     "parent_module": "Sales_Orders",
        #     "subform_field": "Ordered_Items",
        # },
        # "Invoiced_Items": {
        #     "parent_module": "Invoices",
        #     "subform_field": "Invoiced_Items",
        # },
        # "Purchase_Items": {
        #     "parent_module": "Purchase_Orders",
        #     "subform_field": "Purchased_Items",
        # },
    }


    class SubformHandler(TableHandler):
        """
        Handler for Zoho CRM subform/line item tables.

        Subform tables are extracted from array fields within parent records.
        For example, Quoted_Items are extracted from Quotes.Quoted_Items.

        These tables:
        - Don't exist as standalone API endpoints
        - Are extracted by reading parent records and their subform fields
        - Use snapshot ingestion (no individual CDC tracking)
        - Include _parent_id and _parent_module for traceability
        """

        def __init__(self, client, module_handler=None) -> None:
            super().__init__(client)
            self._module_handler = module_handler
            self._schema_cache: dict[str, StructType] = {}

        @staticmethod
        def get_tables() -> dict[str, dict]:
            """Return configuration for all subform tables."""
            return SUBFORM_TABLES

        def get_schema(self, table_name: str, config: dict) -> StructType:
            """
            Get Spark schema for a subform table.

            All line item tables share the LINE_ITEM_SCHEMA since they have
            the same structure (product, quantity, pricing, etc.).

            Args:
                table_name: Name of the subform table
                config: Table configuration (unused for subforms)

            Returns:
                Spark StructType representing the line item schema
            """
            if table_name in self._schema_cache:
                return self._schema_cache[table_name]

            # All line item tables share the same schema
            self._schema_cache[table_name] = LINE_ITEM_SCHEMA
            return LINE_ITEM_SCHEMA

        def get_metadata(self, table_name: str, config: dict) -> dict:
            """
            Get ingestion metadata for a subform table.

            Subform tables use snapshot ingestion since individual items
            don't have their own modification timestamps.

            Args:
                table_name: Name of the subform table
                config: Table configuration (unused for subforms)

            Returns:
                Dictionary with primary_keys and ingestion_type='snapshot'
            """
            # Subforms use snapshot (no individual CDC tracking)
            return {
                "primary_keys": ["id"],
                "ingestion_type": "snapshot",
            }

        def read(
            self,
            table_name: str,
            config: dict,
            start_offset: dict,
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a subform table by extracting from parent records.

            Iterates through all parent records and extracts items from the
            corresponding subform array field. Each item is enriched with
            _parent_id and _parent_module for traceability.

            Args:
                table_name: Name of the subform table
                config: Table configuration with parent_module and subform_field
                start_offset: Offset dictionary (unused - subforms use snapshot)

            Returns:
                Tuple of (records iterator, empty offset dict)
            """
            table_config = SUBFORM_TABLES.get(table_name, {})
            parent_module = table_config.get("parent_module", "")
            subform_field = table_config.get("subform_field", "")

            def records_generator():
                # Get field names for parent module
                field_names = self._get_parent_field_names(parent_module)

                params = {
                    "sort_order": "asc",
                    "sort_by": "Modified_Time",
                }
                if field_names:
                    params["fields"] = ",".join(field_names)

                for parent_record in self.client.paginate(f"/crm/v8/{parent_module}", params=params):
                    parent_id = parent_record.get("id")
                    subform_items = parent_record.get(subform_field, [])

                    if subform_items:
                        for item in subform_items:
                            item["_parent_id"] = parent_id
                            item["_parent_module"] = parent_module
                            yield item

            # Subforms use snapshot - no cursor tracking
            return records_generator(), {}

        def _get_parent_field_names(self, parent_module: str) -> list[str]:
            """
            Get field API names for a parent module.

            Used to request all fields when fetching parent records so that
            subform data is included in the response.

            Args:
                parent_module: Name of the parent Zoho CRM module

            Returns:
                List of field API names, or empty list if unavailable
            """
            if self._module_handler:
                fields = self._module_handler.get_fields(parent_module)
                return [f["api_name"] for f in fields] if fields else []
            return []


    ########################################################
    # src/databricks/labs/community_connector/sources/zoho_crm/zoho_crm.py
    ########################################################

    logger = logging.getLogger(__name__)


    class ZohoCRMLakeflowConnect(LakeflowConnect):
        """
        Zoho CRM connector for Lakeflow/Databricks.

        This class serves as the main entry point and orchestrator for the connector.
        It delegates actual work to specialized handlers for different table types.
        """

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Zoho CRM connector with connection-level options.

            Expected options:
                - client_id: OAuth Client ID from Zoho API Console
                - client_secret: OAuth Client Secret from Zoho API Console
                - refresh_token: Long-lived refresh token obtained from OAuth flow
                - base_url (optional): Zoho accounts URL for OAuth.
                  Defaults to https://accounts.zoho.com
                - initial_load_start_date (optional): Starting point for the first sync.
            """
            client_id = options.get("client_id")
            client_secret = options.get("client_secret")
            refresh_token = options.get("refresh_token")

            if not all([client_id, client_secret, refresh_token]):
                raise ValueError(
                    "Zoho CRM connector requires 'client_id', 'client_secret', "
                    "and 'refresh_token' in the UC connection"
                )

            self.initial_load_start_date = options.get("initial_load_start_date")
            accounts_url = options.get("base_url", "https://accounts.zoho.com")

            self._client = ZohoAPIClient(
                client_id=client_id,
                client_secret=client_secret,
                refresh_token=refresh_token,
                accounts_url=accounts_url,
            )

            self._module_handler = ModuleHandler(self._client)
            self._settings_handler = SettingsHandler(self._client)
            self._subform_handler = SubformHandler(self._client, self._module_handler)
            self._related_handler = RelatedHandler(self._client)

            self._derived_tables = self._build_derived_tables_map()

        def _build_derived_tables_map(self) -> dict[str, tuple[object, dict]]:
            """Build a mapping of derived table names to their handlers and configs."""
            derived = {}

            for name, config in SettingsHandler.get_tables().items():
                derived[name] = (self._settings_handler, {"type": "settings", **config})

            for name, config in SubformHandler.get_tables().items():
                derived[name] = (self._subform_handler, {"type": "subform", **config})

            for name, config in RelatedHandler.get_tables().items():
                derived[name] = (self._related_handler, {"type": "related", **config})

            return derived

        def _get_handler_and_config(self, table_name: str) -> tuple[object, dict]:
            """Get the appropriate handler and configuration for a table."""
            if table_name in self._derived_tables:
                return self._derived_tables[table_name]

            return self._module_handler, {"initial_load_start_date": self.initial_load_start_date}

        def list_tables(self) -> list[str]:
            """List names of all tables (modules) supported by this connector."""
            modules = self._module_handler.get_modules()
            table_names = [m["api_name"] for m in modules]

            table_names.extend(self._derived_tables.keys())

            return sorted(table_names)

        def get_table_schema(self, table_name: str, table_options: dict[str, str]) -> StructType:
            """Fetch the schema of a table dynamically from Zoho CRM."""
            self._validate_table_exists(table_name)
            handler, config = self._get_handler_and_config(table_name)
            return handler.get_schema(table_name, config)

        def read_table_metadata(self, table_name: str, table_options: dict[str, str]) -> dict:
            """Fetch the metadata of a table."""
            self._validate_table_exists(table_name)
            handler, config = self._get_handler_and_config(table_name)
            return handler.get_metadata(table_name, config)

        def read_table(
            self,
            table_name: str,
            start_offset: dict,
            table_options: dict[str, str],
        ) -> tuple[Iterator[dict], dict]:
            """Read records from a Zoho CRM table."""
            self._validate_table_exists(table_name)
            handler, config = self._get_handler_and_config(table_name)
            return handler.read(table_name, config, start_offset)

        def _validate_table_exists(self, table_name: str) -> None:
            """Validate that a table exists and is supported."""
            if table_name in self._derived_tables:
                return

            available_tables = self.list_tables()
            if table_name not in available_tables:
                raise ValueError(
                    f"Table '{table_name}' is not supported. "
                    f"Available tables: {', '.join(available_tables)}"
                )


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = ZohoCRMLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
