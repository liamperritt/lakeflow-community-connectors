# ==============================================================================
# Merged Lakeflow Source: gmail
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Generator,
    Iterator,
    List,
    Optional,
)
import json
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """

        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the records of a table and return an iterator of records and an offset.
            The read starts from the provided start_offset.
            Records returned in the iterator will be one batch of records marked by the
            offset as its end_offset.
            The read_table function could be called multiple times to read the entire table
            in multiple batches and it stops when the same offset is returned again.
            If the table cannot be incrementally read, the offset can be None if we want to
            read the entire table in one batch.
            We could still return some fake offsets (cannot checkpointing) to split the
            table into multiple batches.
            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                An iterator of records in JSON format and an offset.
                DO NOT convert the JSON based on the schema in `get_table_schema` in
                `read_table`.
                records: An iterator of records in JSON format.
                offset: An offset in dict.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                An iterator of deleted records in JSON format and an offset.
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: An offset in dict (same format as read_table).
            """


    ########################################################
    # src/databricks/labs/community_connector/sources/gmail/gmail_schemas.py
    ########################################################

    HEADER_STRUCT = StructType(
        [
            StructField("name", StringType(), True),
            StructField("value", StringType(), True),
        ]
    )

    BODY_STRUCT = StructType(
        [
            StructField("attachmentId", StringType(), True),
            StructField("size", LongType(), True),
            StructField("data", StringType(), True),
        ]
    )

    PART_STRUCT = StructType(
        [
            StructField("partId", StringType(), True),
            StructField("mimeType", StringType(), True),
            StructField("filename", StringType(), True),
            StructField("headers", ArrayType(HEADER_STRUCT), True),
            StructField("body", BODY_STRUCT, True),
        ]
    )

    PAYLOAD_STRUCT = StructType(
        [
            StructField("partId", StringType(), True),
            StructField("mimeType", StringType(), True),
            StructField("filename", StringType(), True),
            StructField("headers", ArrayType(HEADER_STRUCT), True),
            StructField("body", BODY_STRUCT, True),
            StructField("parts", ArrayType(PART_STRUCT), True),
        ]
    )

    # ─── Table Schemas ────────────────────────────────────────────────────────────

    MESSAGES_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("threadId", StringType(), True),
            StructField("labelIds", ArrayType(StringType()), True),
            StructField("snippet", StringType(), True),
            StructField("historyId", StringType(), True),
            StructField("internalDate", StringType(), True),
            StructField("sizeEstimate", LongType(), True),
            StructField("payload", PAYLOAD_STRUCT, True),
        ]
    )

    THREADS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("snippet", StringType(), True),
            StructField("historyId", StringType(), True),
            StructField(
                "messages",
                ArrayType(
                    StructType(
                        [
                            StructField("id", StringType(), True),
                            StructField("threadId", StringType(), True),
                            StructField("labelIds", ArrayType(StringType()), True),
                            StructField("snippet", StringType(), True),
                            StructField("historyId", StringType(), True),
                            StructField("internalDate", StringType(), True),
                        ]
                    )
                ),
                True,
            ),
        ]
    )

    LABELS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField("name", StringType(), True),
            StructField("messageListVisibility", StringType(), True),
            StructField("labelListVisibility", StringType(), True),
            StructField("type", StringType(), True),
            StructField("messagesTotal", LongType(), True),
            StructField("messagesUnread", LongType(), True),
            StructField("threadsTotal", LongType(), True),
            StructField("threadsUnread", LongType(), True),
            StructField(
                "color",
                StructType(
                    [
                        StructField("textColor", StringType(), True),
                        StructField("backgroundColor", StringType(), True),
                    ]
                ),
                True,
            ),
        ]
    )

    DRAFTS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField(
                "message",
                StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("threadId", StringType(), True),
                        StructField("labelIds", ArrayType(StringType()), True),
                        StructField("snippet", StringType(), True),
                        StructField("historyId", StringType(), True),
                        StructField("internalDate", StringType(), True),
                        StructField("sizeEstimate", LongType(), True),
                        StructField("payload", PAYLOAD_STRUCT, True),
                    ]
                ),
                True,
            ),
        ]
    )

    PROFILE_SCHEMA = StructType(
        [
            StructField("emailAddress", StringType(), False),
            StructField("messagesTotal", LongType(), True),
            StructField("threadsTotal", LongType(), True),
            StructField("historyId", StringType(), True),
        ]
    )

    SETTINGS_SCHEMA = StructType(
        [
            StructField("emailAddress", StringType(), False),
            StructField(
                "autoForwarding",
                StructType(
                    [
                        StructField("enabled", BooleanType(), True),
                        StructField("emailAddress", StringType(), True),
                        StructField("disposition", StringType(), True),
                    ]
                ),
                True,
            ),
            StructField(
                "imap",
                StructType(
                    [
                        StructField("enabled", BooleanType(), True),
                        StructField("autoExpunge", BooleanType(), True),
                        StructField("expungeBehavior", StringType(), True),
                        StructField("maxFolderSize", LongType(), True),
                    ]
                ),
                True,
            ),
            StructField(
                "pop",
                StructType(
                    [
                        StructField("accessWindow", StringType(), True),
                        StructField("disposition", StringType(), True),
                    ]
                ),
                True,
            ),
            StructField(
                "language",
                StructType(
                    [
                        StructField("displayLanguage", StringType(), True),
                    ]
                ),
                True,
            ),
            StructField(
                "vacation",
                StructType(
                    [
                        StructField("enableAutoReply", BooleanType(), True),
                        StructField("responseSubject", StringType(), True),
                        StructField("responseBodyPlainText", StringType(), True),
                        StructField("responseBodyHtml", StringType(), True),
                        StructField("restrictToContacts", BooleanType(), True),
                        StructField("restrictToDomain", BooleanType(), True),
                        StructField("startTime", StringType(), True),
                        StructField("endTime", StringType(), True),
                    ]
                ),
                True,
            ),
        ]
    )

    FILTERS_SCHEMA = StructType(
        [
            StructField("id", StringType(), False),
            StructField(
                "criteria",
                StructType(
                    [
                        StructField("from", StringType(), True),
                        StructField("to", StringType(), True),
                        StructField("subject", StringType(), True),
                        StructField("query", StringType(), True),
                        StructField("negatedQuery", StringType(), True),
                        StructField("hasAttachment", BooleanType(), True),
                        StructField("excludeChats", BooleanType(), True),
                        StructField("size", LongType(), True),
                        StructField("sizeComparison", StringType(), True),
                    ]
                ),
                True,
            ),
            StructField(
                "action",
                StructType(
                    [
                        StructField("addLabelIds", ArrayType(StringType()), True),
                        StructField("removeLabelIds", ArrayType(StringType()), True),
                        StructField("forward", StringType(), True),
                    ]
                ),
                True,
            ),
        ]
    )

    FORWARDING_ADDRESSES_SCHEMA = StructType(
        [
            StructField("forwardingEmail", StringType(), False),
            StructField("verificationStatus", StringType(), True),
        ]
    )

    SEND_AS_SCHEMA = StructType(
        [
            StructField("sendAsEmail", StringType(), False),
            StructField("displayName", StringType(), True),
            StructField("replyToAddress", StringType(), True),
            StructField("signature", StringType(), True),
            StructField("isPrimary", BooleanType(), True),
            StructField("isDefault", BooleanType(), True),
            StructField("treatAsAlias", BooleanType(), True),
            StructField("verificationStatus", StringType(), True),
            StructField(
                "smtpMsa",
                StructType(
                    [
                        StructField("host", StringType(), True),
                        StructField("port", LongType(), True),
                        StructField("username", StringType(), True),
                        StructField("securityMode", StringType(), True),
                    ]
                ),
                True,
            ),
        ]
    )

    DELEGATES_SCHEMA = StructType(
        [
            StructField("delegateEmail", StringType(), False),
            StructField("verificationStatus", StringType(), True),
        ]
    )

    # ─── Lookup Tables ────────────────────────────────────────────────────────────

    TABLE_SCHEMAS = {
        "messages": MESSAGES_SCHEMA,
        "threads": THREADS_SCHEMA,
        "labels": LABELS_SCHEMA,
        "drafts": DRAFTS_SCHEMA,
        "profile": PROFILE_SCHEMA,
        "settings": SETTINGS_SCHEMA,
        "filters": FILTERS_SCHEMA,
        "forwarding_addresses": FORWARDING_ADDRESSES_SCHEMA,
        "send_as": SEND_AS_SCHEMA,
        "delegates": DELEGATES_SCHEMA,
    }

    TABLE_METADATA = {
        "messages": {
            "primary_keys": ["id"],
            "cursor_field": "historyId",
            "ingestion_type": "cdc_with_deletes",
        },
        "threads": {
            "primary_keys": ["id"],
            "cursor_field": "historyId",
            "ingestion_type": "cdc_with_deletes",
        },
        "labels": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "drafts": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "profile": {
            "primary_keys": ["emailAddress"],
            "ingestion_type": "snapshot",
        },
        "settings": {
            "primary_keys": ["emailAddress"],
            "ingestion_type": "snapshot",
        },
        "filters": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "forwarding_addresses": {
            "primary_keys": ["forwardingEmail"],
            "ingestion_type": "snapshot",
        },
        "send_as": {
            "primary_keys": ["sendAsEmail"],
            "ingestion_type": "snapshot",
        },
        "delegates": {
            "primary_keys": ["delegateEmail"],
            "ingestion_type": "snapshot",
        },
    }

    SUPPORTED_TABLES = list(TABLE_SCHEMAS.keys())


    ########################################################
    # src/databricks/labs/community_connector/sources/gmail/gmail_utils.py
    ########################################################

    BATCH_SIZE = 50  # Gmail batch API supports up to 100, using 50 for safety
    MAX_WORKERS = 5  # Concurrent workers for parallel fetching


    class GmailApiClient:
        """Handles Gmail API communication: auth, requests, batch, and parallel fetching."""

        BASE_URL = "https://gmail.googleapis.com/gmail/v1"
        BATCH_URL = "https://gmail.googleapis.com/batch/gmail/v1"

        def __init__(
            self,
            client_id: str,
            client_secret: str,
            refresh_token: str,
            user_id: str = "me",
        ) -> None:
            self.client_id = client_id
            self.client_secret = client_secret
            self.refresh_token = refresh_token
            self.user_id = user_id

            self._access_token = None
            self._token_expires_at = 0
            self._session = requests.Session()

        def get_access_token(self) -> str:
            """Exchange refresh token for access token with caching."""
            # Return cached token if still valid (with 60s buffer)
            if self._access_token and time.time() < self._token_expires_at - 60:
                return self._access_token

            response = requests.post(
                "https://oauth2.googleapis.com/token",
                data={
                    "client_id": self.client_id,
                    "client_secret": self.client_secret,
                    "refresh_token": self.refresh_token,
                    "grant_type": "refresh_token",
                },
            )
            response.raise_for_status()
            data = response.json()

            self._access_token = data["access_token"]
            self._token_expires_at = time.time() + data.get("expires_in", 3600)

            return self._access_token

        def get_headers(self) -> Dict[str, str]:
            """Get headers with valid access token."""
            return {
                "Authorization": f"Bearer {self.get_access_token()}",
                "Accept": "application/json",
            }

        def make_request(
            self, method: str, endpoint: str, params: Optional[Dict] = None, retry_count: int = 3
        ) -> Optional[Dict]:
            """Make API request with retry and rate limit handling."""
            url = f"{self.BASE_URL}{endpoint}"

            for attempt in range(retry_count):
                response = self._session.request(
                    method, url, headers=self.get_headers(), params=params
                )

                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    # Rate limited - exponential backoff
                    wait_time = (2**attempt) + 1
                    time.sleep(wait_time)
                    continue
                elif response.status_code == 404:
                    # History ID expired or resource not found
                    return None
                elif response.status_code == 403:
                    # Forbidden - missing OAuth scope or permission
                    return None
                else:
                    response.raise_for_status()

            raise Exception(f"Failed after {retry_count} retries")

        def make_batch_request(
            self, endpoints: List[str], params_list: Optional[List[Dict]] = None
        ) -> List[Dict]:
            """
            Make batch API request for efficient bulk data retrieval.

            Gmail batch API allows up to 100 requests in a single HTTP call,
            reducing network overhead significantly.
            """
            if not endpoints:
                return []

            if params_list is None:
                params_list = [{}] * len(endpoints)

            # Build multipart batch request body
            boundary = "batch_gmail_connector"
            body_parts: List[str] = []

            for i, (endpoint, params) in enumerate(zip(endpoints, params_list)):
                url = f"{self.BASE_URL}{endpoint}"
                if params:
                    query_string = "&".join(f"{k}={v}" for k, v in params.items())
                    url = f"{url}?{query_string}"

                part = f"--{boundary}\r\n"
                part += "Content-Type: application/http\r\n"
                part += f"Content-ID: <item{i}>\r\n\r\n"
                part += f"GET {url}\r\n"
                body_parts.append(part)

            body = "\r\n".join(body_parts) + f"\r\n--{boundary}--"

            headers = self.get_headers()
            headers["Content-Type"] = f"multipart/mixed; boundary={boundary}"

            response = self._session.post(self.BATCH_URL, headers=headers, data=body)

            if response.status_code != 200:
                # Fall back to sequential requests on batch failure
                return self._fetch_sequential(endpoints, params_list)

            return self._parse_batch_response(response.text, boundary)

        def _parse_batch_response(self, response_text: str, boundary: str) -> List[Dict]:
            """Parse multipart batch response."""
            results = []
            parts = response_text.split(f"--{boundary}")

            for part in parts:
                if "Content-Type: application/json" in part or '{"' in part:
                    # Extract JSON from the response part
                    try:
                        json_start = part.find("{")
                        json_end = part.rfind("}") + 1
                        if 0 <= json_start < json_end:
                            json_str = part[json_start:json_end]
                            results.append(json.loads(json_str))
                    except (json.JSONDecodeError, ValueError):
                        continue

            return results

        def _fetch_sequential(
            self, endpoints: List[str], params_list: List[Dict]
        ) -> List[Dict]:
            """Fallback sequential fetch when batch fails."""
            results = []
            for endpoint, params in zip(endpoints, params_list):
                result = self.make_request("GET", endpoint, params)
                if result:
                    results.append(result)
            return results

        def fetch_details_parallel(
            self, ids: List[str], fetch_func, max_workers: int = MAX_WORKERS
        ) -> Generator[Dict, None, None]:
            """
            Fetch details in parallel using thread pool.
            Yields results as they complete for true streaming.
            """
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(fetch_func, id_): id_ for id_ in ids
                }
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        if result:
                            yield result
                    except Exception:
                        # Skip failed fetches, continue with others
                        continue


    ########################################################
    # src/databricks/labs/community_connector/sources/gmail/gmail.py
    ########################################################

    class GmailLakeflowConnect(LakeflowConnect):
        """Gmail connector implementing the LakeflowConnect interface with 100% API coverage."""

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Gmail connector with OAuth 2.0 credentials.

            Expected options:
                - client_id: OAuth 2.0 client ID from Google Cloud Console
                - client_secret: OAuth 2.0 client secret
                - refresh_token: Long-lived refresh token obtained via OAuth flow
                - user_id (optional): User email or 'me' (default: 'me')
            """
            self.client_id = options.get("client_id")
            self.client_secret = options.get("client_secret")
            self.refresh_token = options.get("refresh_token")
            self.user_id = options.get("user_id", "me")

            if not self.client_id:
                raise ValueError("Gmail connector requires 'client_id' in options")
            if not self.client_secret:
                raise ValueError("Gmail connector requires 'client_secret' in options")
            if not self.refresh_token:
                raise ValueError("Gmail connector requires 'refresh_token' in options")

            self.api = GmailApiClient(
                self.client_id, self.client_secret, self.refresh_token, self.user_id
            )

        # ─── Interface Methods ────────────────────────────────────────────────────

        def list_tables(self) -> list[str]:
            """Return the list of available Gmail tables."""
            return SUPPORTED_TABLES.copy()

        def get_table_schema(self, table_name: str, table_options: Dict[str, str]) -> StructType:
            """Fetch the schema of a table."""
            self._validate_table(table_name)
            return TABLE_SCHEMAS[table_name]

        def read_table_metadata(self, table_name: str, table_options: Dict[str, str]) -> Dict:
            """Fetch the metadata of a table."""
            self._validate_table(table_name)
            return TABLE_METADATA[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table with streaming iteration.

            For messages and threads with a start_offset containing historyId,
            uses the History API for incremental reads. Otherwise, performs
            a full list operation.
            """
            self._validate_table(table_name)

            read_map = {
                "messages": self._read_messages,
                "threads": self._read_threads,
                "labels": self._read_labels,
                "drafts": self._read_drafts,
                "profile": self._read_profile,
                "settings": self._read_settings,
                "filters": self._read_filters,
                "forwarding_addresses": self._read_forwarding_addresses,
                "send_as": self._read_send_as,
                "delegates": self._read_delegates,
            }
            return read_map[table_name](start_offset, table_options)

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read deleted records from Gmail using History API.

            Gmail History API provides messagesDeleted events for tracking deletions.
            This is only applicable for messages and threads (CDC tables).
            """
            self._validate_table(table_name)

            # Only messages and threads support delete tracking
            if table_name not in ("messages", "threads"):
                return iter([]), {}

            start_history_id = start_offset.get("historyId") if start_offset else None
            if not start_history_id:
                return iter([]), {}

            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageDeleted",
            }

            page_token = None
            latest_history_id = start_history_id
            deleted_records = []
            seen_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request("GET", f"/users/{self.user_id}/history", params=params)

                if response is None:
                    break

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                deleted_records.extend(
                    self._collect_deleted_records(
                        table_name, response.get("history", []), seen_ids, latest_history_id
                    )
                )

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            next_offset = {"historyId": latest_history_id}
            return iter(deleted_records), next_offset

        def _collect_deleted_records(
            self, table_name: str, history_records: list, seen_ids: set, latest_history_id: str
        ) -> list[dict]:
            """Extract deleted record entries from history records."""
            deleted_records = []
            for history_record in history_records:
                record_history_id = history_record.get("id", latest_history_id)
                for deleted in history_record.get("messagesDeleted", []):
                    msg = deleted.get("message", {})
                    msg_id = msg.get("id")
                    if not msg_id:
                        continue
                    if table_name == "messages" and msg_id not in seen_ids:
                        seen_ids.add(msg_id)
                        deleted_records.append(
                            {
                                "id": msg_id,
                                "threadId": msg.get("threadId"),
                                "historyId": record_history_id,
                            }
                        )
                    elif table_name == "threads":
                        thread_id = msg.get("threadId")
                        if thread_id and thread_id not in seen_ids:
                            seen_ids.add(thread_id)
                            deleted_records.append({"id": thread_id, "historyId": record_history_id})
            return deleted_records

        # ─── Helpers ──────────────────────────────────────────────────────────────

        def _validate_table(self, table_name: str) -> None:
            """Validate that the table is supported."""
            if table_name not in SUPPORTED_TABLES:
                raise ValueError(
                    f"Unsupported table: '{table_name}'. Supported tables are: {SUPPORTED_TABLES}"
                )

        # ─── Table Readers ────────────────────────────────────────────────────────

        def _read_messages(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read messages using streaming list + batch get pattern."""
            start_history_id = start_offset.get("historyId") if start_offset else None

            if start_history_id:
                return self._read_messages_incremental(start_history_id, table_options)
            return self._read_messages_streaming(table_options)

        def _read_messages_streaming(
            self,
            table_options: Dict[str, str],
        ) -> (Iterator[dict], dict):
            """Stream messages with sequential fetching for reliability."""
            max_results = int(table_options.get("maxResults", "100"))
            query = table_options.get("q")
            label_ids = table_options.get("labelIds")
            include_spam_trash = table_options.get("includeSpamTrash", "false").lower() == "true"

            params = {"maxResults": min(max_results, 500)}
            if query:
                params["q"] = query
            if label_ids:
                params["labelIds"] = label_ids
            if include_spam_trash:
                params["includeSpamTrash"] = "true"

            state = {"latest_history_id": None}
            format_type = table_options.get("format", "full")

            def fetch_message(mid):
                return self.api.make_request(
                    "GET",
                    f"/users/{self.user_id}/messages/{mid}",
                    {"format": format_type},
                )

            all_messages = []
            page_token = None

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request(
                    "GET", f"/users/{self.user_id}/messages", params=params
                )

                if not response or "messages" not in response:
                    break

                message_ids = [m["id"] for m in response.get("messages", [])]

                # Fetch messages in parallel for better performance
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    results = list(executor.map(fetch_message, message_ids))

                for msg_detail in results:
                    if msg_detail:
                        if msg_detail.get("historyId"):
                            if (
                                not state["latest_history_id"]
                                or msg_detail["historyId"] > state["latest_history_id"]
                            ):
                                state["latest_history_id"] = msg_detail["historyId"]
                        all_messages.append(msg_detail)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            next_offset = (
                {"historyId": state["latest_history_id"]}
                if state["latest_history_id"]
                else {}
            )
            return iter(all_messages), next_offset

        def _read_messages_incremental(
            self, start_history_id: str, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read messages incrementally using History API with batch fetching."""
            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageAdded",
            }

            page_token = None
            latest_history_id = start_history_id
            all_message_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request("GET", f"/users/{self.user_id}/history", params=params)

                if response is None:
                    return self._read_messages_streaming(table_options)

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                for history_record in response.get("history", []):
                    for added in history_record.get("messagesAdded", []):
                        msg_id = added.get("message", {}).get("id")
                        if msg_id:
                            all_message_ids.add(msg_id)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            all_messages = []
            message_ids = list(all_message_ids)
            format_type = table_options.get("format", "full")

            for i in range(0, len(message_ids), BATCH_SIZE):
                batch_ids = message_ids[i : i + BATCH_SIZE]
                endpoints = [f"/users/{self.user_id}/messages/{mid}" for mid in batch_ids]
                params_list = [{"format": format_type}] * len(batch_ids)

                batch_results = self.api.make_batch_request(endpoints, params_list)
                all_messages.extend([r for r in batch_results if r])

            next_offset = {"historyId": latest_history_id}
            return iter(all_messages), next_offset

        def _read_threads(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read threads using streaming list + batch get pattern."""
            start_history_id = start_offset.get("historyId") if start_offset else None

            if start_history_id:
                return self._read_threads_incremental(start_history_id, table_options)
            return self._read_threads_streaming(table_options)

        def _read_threads_streaming(
            self,
            table_options: Dict[str, str],
        ) -> (Iterator[dict], dict):
            """Stream threads with parallel fetching for performance."""
            max_results = int(table_options.get("maxResults", "100"))
            query = table_options.get("q")
            label_ids = table_options.get("labelIds")
            include_spam_trash = table_options.get("includeSpamTrash", "false").lower() == "true"

            params = {"maxResults": min(max_results, 500)}
            if query:
                params["q"] = query
            if label_ids:
                params["labelIds"] = label_ids
            if include_spam_trash:
                params["includeSpamTrash"] = "true"

            state = {"latest_history_id": None}
            all_threads = []
            page_token = None
            format_type = table_options.get("format", "full")

            def fetch_thread(tid):
                return self.api.make_request(
                    "GET",
                    f"/users/{self.user_id}/threads/{tid}",
                    {"format": format_type},
                )

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request("GET", f"/users/{self.user_id}/threads", params=params)

                if not response or "threads" not in response:
                    break

                thread_ids = [t["id"] for t in response.get("threads", [])]

                # Fetch threads in parallel for better performance
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    results = list(executor.map(fetch_thread, thread_ids))

                for thread_detail in results:
                    if thread_detail:
                        if thread_detail.get("historyId"):
                            if (
                                not state["latest_history_id"]
                                or thread_detail["historyId"] > state["latest_history_id"]
                            ):
                                state["latest_history_id"] = thread_detail["historyId"]
                        all_threads.append(thread_detail)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            next_offset = (
                {"historyId": state["latest_history_id"]}
                if state["latest_history_id"]
                else {}
            )
            return iter(all_threads), next_offset

        def _read_threads_incremental(
            self, start_history_id: str, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read threads incrementally using History API."""
            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageAdded",
            }

            page_token = None
            latest_history_id = start_history_id
            all_thread_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request("GET", f"/users/{self.user_id}/history", params=params)

                if response is None:
                    return self._read_threads_streaming(table_options)

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                for history_record in response.get("history", []):
                    for added in history_record.get("messagesAdded", []):
                        thread_id = added.get("message", {}).get("threadId")
                        if thread_id:
                            all_thread_ids.add(thread_id)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            all_threads = []
            thread_ids = list(all_thread_ids)
            format_type = table_options.get("format", "full")

            for i in range(0, len(thread_ids), BATCH_SIZE):
                batch_ids = thread_ids[i : i + BATCH_SIZE]
                endpoints = [f"/users/{self.user_id}/threads/{tid}" for tid in batch_ids]
                params_list = [{"format": format_type}] * len(batch_ids)

                batch_results = self.api.make_batch_request(endpoints, params_list)
                all_threads.extend([r for r in batch_results if r])

            next_offset = {"historyId": latest_history_id}
            return iter(all_threads), next_offset

        def _read_labels(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all labels (snapshot mode)."""
            response = self.api.make_request("GET", f"/users/{self.user_id}/labels")

            if not response or "labels" not in response:
                return iter([]), {}

            # Get basic label list first
            labels = response.get("labels", [])

            # Fetch full details for each label to get counts and colors
            all_labels = []
            for label in labels:
                label_id = label.get("id")
                if label_id:
                    detail = self.api.make_request("GET", f"/users/{self.user_id}/labels/{label_id}")
                    if detail:
                        all_labels.append(detail)
                    else:
                        # Fallback to basic info if detail fetch fails
                        all_labels.append(label)

            return iter(all_labels), {}

        def _read_drafts(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all drafts (snapshot mode) with parallel fetching."""
            params = {"maxResults": int(table_options.get("maxResults", "100"))}

            all_drafts = []
            page_token = None
            format_type = table_options.get("format", "full")

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self.api.make_request("GET", f"/users/{self.user_id}/drafts", params=params)

                if not response or "drafts" not in response:
                    break

                draft_ids = [d["id"] for d in response.get("drafts", [])]

                # Fetch drafts in parallel for better performance
                def fetch_draft(draft_id):
                    return self.api.make_request(
                        "GET",
                        f"/users/{self.user_id}/drafts/{draft_id}",
                        {"format": format_type},
                    )

                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    results = list(executor.map(fetch_draft, draft_ids))
                    all_drafts.extend([r for r in results if r])

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            return iter(all_drafts), {}

        def _read_profile(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read user profile (single record)."""
            response = self.api.make_request("GET", f"/users/{self.user_id}/profile")

            if not response:
                return iter([]), {}

            return iter([response]), {}

        def _read_settings(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all user settings combined into a single record."""
            # Fetch all settings in parallel
            endpoints = [
                f"/users/{self.user_id}/settings/autoForwarding",
                f"/users/{self.user_id}/settings/imap",
                f"/users/{self.user_id}/settings/pop",
                f"/users/{self.user_id}/settings/language",
                f"/users/{self.user_id}/settings/vacation",
            ]

            results = self.api.make_batch_request(endpoints)

            # Get email address from profile
            profile = self.api.make_request("GET", f"/users/{self.user_id}/profile")
            email_address = profile.get("emailAddress", self.user_id) if profile else self.user_id

            # Combine all settings into one record
            settings = {
                "emailAddress": email_address,
                "autoForwarding": results[0] if len(results) > 0 else None,
                "imap": results[1] if len(results) > 1 else None,
                "pop": results[2] if len(results) > 2 else None,
                "language": results[3] if len(results) > 3 else None,
                "vacation": results[4] if len(results) > 4 else None,
            }

            return iter([settings]), {}

        def _read_filters(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all email filters."""
            response = self.api.make_request("GET", f"/users/{self.user_id}/settings/filters")

            if not response or "filter" not in response:
                return iter([]), {}

            return iter(response.get("filter", [])), {}

        def _read_forwarding_addresses(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all forwarding addresses."""
            response = self.api.make_request(
                "GET", f"/users/{self.user_id}/settings/forwardingAddresses"
            )

            if not response or "forwardingAddresses" not in response:
                return iter([]), {}

            return iter(response.get("forwardingAddresses", [])), {}

        def _read_send_as(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all send-as aliases."""
            response = self.api.make_request("GET", f"/users/{self.user_id}/settings/sendAs")

            if not response or "sendAs" not in response:
                return iter([]), {}

            return iter(response.get("sendAs", [])), {}

        def _read_delegates(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all delegates."""
            response = self.api.make_request("GET", f"/users/{self.user_id}/settings/delegates")

            if not response or "delegates" not in response:
                return iter([]), {}

            return iter(response.get("delegates", [])), {}


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = GmailLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
