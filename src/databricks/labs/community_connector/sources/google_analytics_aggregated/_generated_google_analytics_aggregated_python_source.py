# ==============================================================================
# Merged Lakeflow Source: google_analytics_aggregated
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from decimal import Decimal
from pathlib import Path
from typing import Any, Iterator
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect(ABC):
        """Base interface that each source connector must implement.

        Subclass this and implement all abstract methods to create a connector that
        integrates with the community connector library and ingestion pipeline.
        """

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """
            self.options = options

        @abstractmethod
        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        @abstractmethod
        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        @abstractmethod
        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        @abstractmethod
        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records and an offset.

            The framework calls this method repeatedly to paginate through data.
            start_offset is None only on the very first call of the very first run
            of a connector. On subsequent runs the framework resumes from the last
            checkpointed offset, so start_offset will already be populated. Each
            call returns (records, end_offset). The framework passes end_offset as
            start_offset to the next call. Pagination stops when the returned
            offset equals start_offset (i.e., no more data).

            For tables that cannot be incrementally read, return None as the offset to
            read the entire table in one batch. Non-checkpointable synthetic offsets can
            be used to split the data into multiple batches.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from. None only on the
                    first call of the first run; on subsequent runs it carries the
                    checkpointed offset from the previous run.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of records as JSON-compatible dicts. Do NOT convert
                    values according to get_table_schema(); the framework handles that.
                offset: A dict representing the position after this batch.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            This method follows the same pagination and offset protocol as read_table:
            the framework calls it repeatedly, passing the previous end_offset as
            start_offset, until the returned offset equals start_offset.

            Override this method if any of your tables use ingestion_type
            "cdc_with_deletes". The default implementation raises NotImplementedError.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: A dict (same format as read_table).
            """
            raise NotImplementedError(
                "read_table_deletes() must be implemented when ingestion_type is 'cdc_with_deletes'"
            )


    ########################################################
    # src/databricks/labs/community_connector/sources/google_analytics_aggregated/google_analytics_aggregated.py
    ########################################################

    try:
        from google.oauth2 import service_account
        from google.auth.transport.requests import Request
    except ImportError:
        raise ImportError(
            "google-auth library is required for Google Analytics connector. "
            "Install it with: pip install google-auth"
        )


    class GoogleAnalyticsAggregatedLakeflowConnect(LakeflowConnect):
        # Class-level cache for prebuilt reports (loaded once)
        _prebuilt_reports_cache = None

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Google Analytics Aggregated Data connector.

            Expected options:
                - property_ids: JSON array of GA4 property IDs (numeric strings)
                - credentials_json: Service account JSON credentials
            """
            self.property_ids = self._parse_property_ids(options)
            self.base_url = "https://analyticsdata.googleapis.com/v1beta"
            self.credentials = self._parse_credentials(options)

            # Create Google service account credentials
            scopes = ["https://www.googleapis.com/auth/analytics.readonly"]
            try:
                self._credentials = (
                    service_account.Credentials.from_service_account_info(
                        self.credentials, scopes=scopes
                    )
                )
            except Exception as e:
                raise ValueError(
                    f"Failed to create credentials from service account: {e}"
                )

            # Fetch and cache metadata for type information
            self._metadata_cache = None

        @staticmethod
        def _parse_property_ids(options):
            """Parse and validate property_ids from connection options."""
            property_ids_json = options.get("property_ids")

            if not property_ids_json:
                raise ValueError(
                    "Google Analytics connector requires 'property_ids' (list) "
                    "in options. Example: property_ids=['123456789']"
                )

            try:
                if isinstance(property_ids_json, str):
                    property_ids = json.loads(property_ids_json)
                else:
                    property_ids = property_ids_json

                if not isinstance(property_ids, list) or len(property_ids) == 0:
                    raise ValueError("property_ids must be a non-empty list")

                for pid in property_ids:
                    if not isinstance(pid, str):
                        raise ValueError(
                            f"All property IDs must be strings, got: {type(pid)}"
                        )

                return property_ids
            except (json.JSONDecodeError, ValueError) as e:
                raise ValueError(f"Invalid 'property_ids': {e}")

        @staticmethod
        def _parse_credentials(options):
            """Parse and validate service account credentials."""
            credentials_json = options.get("credentials_json")
            if not credentials_json:
                raise ValueError(
                    "Google Analytics connector requires 'credentials_json' "
                    "in options"
                )

            if isinstance(credentials_json, str):
                try:
                    credentials = json.loads(credentials_json)
                except json.JSONDecodeError as e:
                    raise ValueError(
                        f"Invalid JSON in 'credentials_json': {e}"
                    )
            else:
                credentials = credentials_json

            required_fields = [
                "type", "client_email", "private_key", "token_uri"
            ]
            missing_fields = [
                f for f in required_fields if f not in credentials
            ]
            if missing_fields:
                raise ValueError(
                    f"Service account credentials missing required fields: "
                    f"{missing_fields}"
                )

            return credentials

        @classmethod
        def _load_prebuilt_reports(cls) -> dict:
            """
            Load prebuilt report configurations from prebuilt_reports.json.
            Uses class-level caching to avoid repeated file reads.

            Returns:
                Dictionary mapping report names to their configurations
            """
            if cls._prebuilt_reports_cache is not None:
                return cls._prebuilt_reports_cache

            # Find the prebuilt_reports.json file relative to this module
            current_file = Path(__file__).resolve()
            prebuilt_reports_path = current_file.parent / "prebuilt_reports.json"

            if not prebuilt_reports_path.exists():
                # If file doesn't exist, return empty dict (all reports are custom)
                cls._prebuilt_reports_cache = {}
                return cls._prebuilt_reports_cache

            try:
                with open(prebuilt_reports_path, 'r', encoding='utf-8') as f:
                    cls._prebuilt_reports_cache = json.load(f)
                return cls._prebuilt_reports_cache
            except Exception as e:
                raise ValueError(
                    f"Failed to load prebuilt reports from "
                    f"{prebuilt_reports_path}: {e}"
                )

        def _resolve_table_options(
            self, table_options: dict[str, str]
        ) -> dict[str, str]:
            """
            Resolve table options by merging prebuilt report configuration
            with user overrides.

            If table_options contains 'prebuilt_report', loads that report's
            configuration and merges it with any additional options provided
            by the user.
            """
            prebuilt_report_name = table_options.get("prebuilt_report")

            if not prebuilt_report_name:
                # No prebuilt report specified, return options as-is
                return table_options

            # Load prebuilt reports
            prebuilt_reports = self._load_prebuilt_reports()

            if prebuilt_report_name not in prebuilt_reports:
                available_reports = ', '.join(sorted(prebuilt_reports.keys()))
                raise ValueError(
                    f"Prebuilt report '{prebuilt_report_name}' not found. "
                    f"Available prebuilt reports: {available_reports}"
                )

            # Start with prebuilt config
            prebuilt_config = prebuilt_reports[prebuilt_report_name].copy()

            # Merge with user-provided options (user options take precedence)
            resolved_options = prebuilt_config.copy()
            for key, value in table_options.items():
                if key != "prebuilt_report":
                    resolved_options[key] = value

            return resolved_options

        def _get_access_token(self) -> str:
            """
            Obtain or refresh the OAuth access token using Google's official
            auth library.
            """
            if not self._credentials.valid:
                auth_request = Request()
                self._credentials.refresh(auth_request)

            return self._credentials.token

        def _fetch_metadata(self) -> dict:
            """
            Fetch metadata for dimensions and metrics from the GA4 Data API.
            Returns a dictionary with:
              - 'metric_types': mapping of metric names to their types
              - 'available_dimensions': set of valid dimension names
              - 'available_metrics': set of valid metric names

            This is called once and cached for type inference and validation.
            For multi-property connectors, fetches metadata from the first
            property (standard dimensions/metrics are the same across all).
            """
            if self._metadata_cache is not None:
                return self._metadata_cache

            first_property_id = self.property_ids[0]
            url = f"{self.base_url}/properties/{first_property_id}/metadata"
            headers = {
                "Authorization": f"Bearer {self._get_access_token()}",
                "Content-Type": "application/json",
            }

            try:
                response = requests.get(url, headers=headers, timeout=60)
                response.raise_for_status()
                metadata = response.json()

                metric_types = {}
                available_dimensions = set()
                available_metrics = set()

                for dimension in metadata.get("dimensions", []):
                    api_name = dimension.get("apiName")
                    if api_name:
                        available_dimensions.add(api_name)

                for metric in metadata.get("metrics", []):
                    api_name = metric.get("apiName")
                    metric_type = metric.get("type")
                    if api_name:
                        available_metrics.add(api_name)
                        if metric_type:
                            metric_types[api_name] = metric_type

                self._metadata_cache = {
                    "metric_types": metric_types,
                    "available_dimensions": available_dimensions,
                    "available_metrics": available_metrics,
                }
                return self._metadata_cache

            except requests.exceptions.RequestException as e:
                raise RuntimeError(
                    f"Failed to fetch metadata from Google Analytics API: {e}"
                )

        def _get_pyspark_type_for_metric(self, metric_type: str):
            """Map Google Analytics metric type to PySpark data type."""
            if metric_type == "TYPE_INTEGER":
                return LongType()
            elif metric_type == "TYPE_MILLISECONDS":
                return LongType()
            elif metric_type in [
                "TYPE_FLOAT",
                "TYPE_CURRENCY",
                "TYPE_SECONDS",
                "TYPE_MINUTES",
                "TYPE_HOURS",
                "TYPE_FEET",
                "TYPE_MILES",
                "TYPE_METERS",
                "TYPE_KILOMETERS",
                "TYPE_STANDARD",
            ]:
                return DoubleType()
            else:
                return StringType()

        def _validate_dimensions_and_metrics(
            self, dimensions: list, metrics: list
        ):
            """
            Validate that requested dimensions and metrics exist in the
            property metadata and that they don't exceed API limits.

            API Limits validated:
            - Maximum 9 dimensions per request
            - Maximum 10 metrics per request
            """
            metadata = self._fetch_metadata()
            available_dimensions = metadata.get("available_dimensions", set())
            available_metrics = metadata.get("available_metrics", set())

            # Check for API limits (validated empirically via test suite)
            max_dimensions = 9
            max_metrics = 10

            dimension_count = len(dimensions)
            metric_count = len(metrics)

            unknown_dimensions = [
                d for d in dimensions if d not in available_dimensions
            ]
            unknown_metrics = [
                m for m in metrics if m not in available_metrics
            ]

            has_errors = (
                unknown_dimensions or unknown_metrics
                or dimension_count > max_dimensions
                or metric_count > max_metrics
            )
            if not has_errors:
                return

            over_limits = (
                dimension_count > max_dimensions
                or metric_count > max_metrics
            )
            error_parts = self._build_validation_error({
                "dimensions": dimensions,
                "metrics": metrics,
                "dimension_count": dimension_count,
                "metric_count": metric_count,
                "max_dimensions": max_dimensions,
                "max_metrics": max_metrics,
                "unknown_dimensions": unknown_dimensions,
                "unknown_metrics": unknown_metrics,
                "available_dimensions": available_dimensions,
                "available_metrics": available_metrics,
                "over_limits": over_limits,
                "property_id": self.property_ids[0],
            })
            raise ValueError("".join(error_parts))

        @staticmethod
        def _build_validation_error(ctx):
            """Build a comprehensive validation error message."""
            error_parts = ["Invalid report configuration:"]

            if ctx["dimension_count"] > ctx["max_dimensions"]:
                error_parts.append(
                    f"\n  Too many dimensions: {ctx['dimension_count']} "
                    f"(max {ctx['max_dimensions']})"
                )
                error_parts.append(
                    f"    The GA4 API limits requests to "
                    f"{ctx['max_dimensions']} dimensions."
                )
                error_parts.append(
                    f"    Your request has: {ctx['dimensions']}"
                )

            if ctx["metric_count"] > ctx["max_metrics"]:
                error_parts.append(
                    f"\n  Too many metrics: {ctx['metric_count']} "
                    f"(max {ctx['max_metrics']})"
                )
                error_parts.append(
                    f"    The GA4 API limits requests to "
                    f"{ctx['max_metrics']} metrics."
                )
                error_parts.append(
                    f"    Your request has: {ctx['metrics']}"
                )

            if ctx["unknown_dimensions"]:
                error_parts.append(
                    f"\n  Unknown dimensions: {ctx['unknown_dimensions']}"
                )
                sample = sorted(list(ctx["available_dimensions"]))[:10]
                error_parts.append(
                    f"    Available dimensions include: {sample}..."
                )

            if ctx["unknown_metrics"]:
                error_parts.append(
                    f"\n  Unknown metrics: {ctx['unknown_metrics']}"
                )
                sample = sorted(list(ctx["available_metrics"]))[:10]
                error_parts.append(
                    f"    Available metrics include: {sample}..."
                )

            error_parts.append(
                "\n\nTo see all available dimensions and metrics "
                "for your property:"
            )
            error_parts.append(
                f"\n  GET https://analyticsdata.googleapis.com/v1beta/"
                f"properties/{ctx['property_id']}/metadata"
            )

            if ctx["over_limits"]:
                error_parts.append(
                    "\n\nTo work around dimension/metric limits:"
                )
                error_parts.append(
                    "\n  - Split your report into multiple smaller "
                    "reports"
                )
                error_parts.append(
                    "\n  - Prioritize the most important dimensions "
                    "and metrics"
                )

            return error_parts

        def _make_api_request(
            self, endpoint: str, body: dict, property_id: str,
            retry_count: int = 3
        ) -> dict:
            """
            Make an authenticated API request to Google Analytics Data API
            with retry logic.
            """
            url = f"{self.base_url}/properties/{property_id}:{endpoint}"
            access_token = self._get_access_token()

            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json",
            }

            for attempt in range(retry_count):
                response = requests.post(
                    url, headers=headers, json=body, timeout=120
                )

                if response.status_code == 200:
                    return response.json()

                elif response.status_code == 429:
                    wait_time = (2**attempt) * 5
                    retry_after = response.headers.get("Retry-After")
                    if retry_after:
                        wait_time = int(retry_after)

                    if attempt < retry_count - 1:
                        time.sleep(wait_time)
                        continue
                    raise RuntimeError(
                        f"Rate limit exceeded after {retry_count} retries: "
                        f"{response.text}"
                    )

                elif response.status_code == 401:
                    if attempt == 0:
                        auth_request = Request()
                        self._credentials.refresh(auth_request)
                        access_token = self._credentials.token
                        headers["Authorization"] = f"Bearer {access_token}"
                        continue
                    raise RuntimeError(
                        f"Authentication failed: {response.status_code} - "
                        f"{response.text}"
                    )

                elif response.status_code == 403:
                    raise RuntimeError(
                        f"Permission denied. Ensure service account has "
                        f"access to property {property_id}: {response.text}"
                    )

                else:
                    raise RuntimeError(
                        f"API request failed: {response.status_code} - "
                        f"{response.text}"
                    )

            raise RuntimeError(
                f"API request failed after {retry_count} retries"
            )

        def _get_effective_options(
            self, table_name, table_options, merge_overrides=False
        ):
            """
            Resolve table options by checking prebuilt reports and merging.

            For prebuilt reports identified by table_name, loads their config.
            If merge_overrides is True, user-provided table_options are merged
            on top (used by read_table for runtime settings like start_date).
            Prebuilt primary_keys are normalized to include property_id.
            """
            prebuilt_reports = self._load_prebuilt_reports()
            is_prebuilt = table_name in prebuilt_reports
            has_custom_dims = "dimensions" in table_options

            if is_prebuilt and not has_custom_dims:
                config = prebuilt_reports[table_name].copy()
                # Normalize: prebuilt primary_keys don't include property_id
                if config.get("primary_keys"):
                    config["primary_keys"] = (
                        ["property_id"] + config["primary_keys"]
                    )
                if merge_overrides:
                    for key, value in table_options.items():
                        config[key] = value
                return config

            if is_prebuilt and has_custom_dims:
                print(
                    f"⚠️  WARNING: Custom config for '{table_name}' "
                    f"(shadowing prebuilt report)"
                )
                print("    Consider using a different source_table name.")

            return self._resolve_table_options(table_options)

        @staticmethod
        def _parse_dimensions_and_metrics(table_options):
            """Parse and validate dimensions and metrics from table options."""
            dimensions_json = table_options.get("dimensions", "[]")
            metrics_json = table_options.get("metrics", "[]")

            try:
                dimensions = json.loads(dimensions_json)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON in 'dimensions' option: {dimensions_json}"
                )

            try:
                metrics = json.loads(metrics_json)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON in 'metrics' option: {metrics_json}"
                )

            if not isinstance(dimensions, list):
                raise ValueError("'dimensions' must be a JSON array of strings")

            if not isinstance(metrics, list) or len(metrics) == 0:
                raise ValueError(
                    "'metrics' must be a JSON array of strings with at least "
                    "one metric"
                )

            return dimensions, metrics

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.

            Returns the list of available prebuilt reports. Users can:
            1. Use a prebuilt report name directly as the source_table
            2. Define custom reports with any name via table_options
            """
            prebuilt_reports = self._load_prebuilt_reports()
            return list(prebuilt_reports.keys())

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            Supports prebuilt reports (by table_name) and custom reports
            (via table_options with dimensions/metrics).
            """
            table_options = self._get_effective_options(
                table_name, table_options
            )
            dimensions, metrics = self._parse_dimensions_and_metrics(
                table_options
            )

            # Validate dimensions and metrics exist
            self._validate_dimensions_and_metrics(dimensions, metrics)

            # Fetch metadata to get proper types for metrics
            metadata = self._fetch_metadata()
            metric_types = metadata.get("metric_types", {})

            # Build schema fields
            schema_fields = [
                StructField("property_id", StringType(), False)
            ]

            # Only pure date dimensions use DateType
            date_dimensions = ["date", "firstSessionDate"]
            for dim in dimensions:
                if dim in date_dimensions:
                    schema_fields.append(
                        StructField(dim, DateType(), True)
                    )
                else:
                    schema_fields.append(
                        StructField(dim, StringType(), True)
                    )

            for metric in metrics:
                metric_type = metric_types.get(metric)
                if metric_type:
                    pyspark_type = self._get_pyspark_type_for_metric(
                        metric_type
                    )
                    schema_fields.append(
                        StructField(metric, pyspark_type, True)
                    )
                else:
                    schema_fields.append(
                        StructField(metric, StringType(), True)
                    )

            return StructType(schema_fields)

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.

            Supports prebuilt reports (by table_name) and custom reports
            (via table_options).
            """
            table_options = self._get_effective_options(
                table_name, table_options
            )

            dimensions_json = table_options.get("dimensions", "[]")
            try:
                dimensions = json.loads(dimensions_json)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON in 'dimensions' option: {dimensions_json}"
                )

            if not isinstance(dimensions, list):
                raise ValueError("'dimensions' must be a JSON array of strings")

            # Determine primary keys
            if "primary_keys" in table_options:
                primary_keys = table_options.get("primary_keys")
            else:
                primary_keys = ["property_id"] + dimensions

            metadata = {"primary_keys": primary_keys}
            if "date" in dimensions:
                metadata["cursor_field"] = "date"
                # Use "cdc" to enable MERGE behavior for lookback_days
                metadata["ingestion_type"] = "cdc"
            else:
                metadata["ingestion_type"] = "snapshot"

            return metadata

        def _parse_metric_value(self, value_str: str, metric_type: str):
            """Parse metric value string according to its type from the API."""
            try:
                if metric_type == "TYPE_INTEGER":
                    return int(value_str)
                elif metric_type == "TYPE_MILLISECONDS":
                    return int(value_str)
                elif metric_type in [
                    "TYPE_FLOAT",
                    "TYPE_CURRENCY",
                    "TYPE_SECONDS",
                    "TYPE_MINUTES",
                    "TYPE_HOURS",
                    "TYPE_FEET",
                    "TYPE_MILES",
                    "TYPE_METERS",
                    "TYPE_KILOMETERS",
                    "TYPE_STANDARD",
                ]:
                    return float(value_str)
                else:
                    return value_str
            except (ValueError, TypeError):
                return None

        @staticmethod
        def _parse_date_value(dim_value):
            """Parse YYYYMMDD string to YYYY-MM-DD format."""
            try:
                year = int(dim_value[0:4])
                month = int(dim_value[4:6])
                day = int(dim_value[6:8])
                return f"{year:04d}-{month:02d}-{day:02d}"
            except (ValueError, IndexError):
                return None

        def _build_report_request(
            self, dimensions, metrics, table_options, start_offset
        ):
            """Build the GA4 runReport API request body."""
            lookback_days = int(table_options.get("lookback_days", 3))
            page_size = min(
                int(table_options.get("page_size", 10000)), 100000
            )

            if start_offset and "last_date" in start_offset:
                last_date_str = start_offset["last_date"]
                last_date = datetime.strptime(last_date_str, "%Y-%m-%d")
                start_date = last_date - timedelta(days=lookback_days)
                start_date_str = start_date.strftime("%Y-%m-%d")
                end_date_str = "today"
            else:
                start_date_str = table_options.get("start_date", "30daysAgo")
                end_date_str = "today"

            date_ranges = [
                {"startDate": start_date_str, "endDate": end_date_str}
            ]
            max_date_ranges = 4
            if len(date_ranges) > max_date_ranges:
                raise ValueError(
                    f"Too many date ranges: {len(date_ranges)} "
                    f"(maximum {max_date_ranges}). The Google Analytics "
                    f"Data API limits requests to {max_date_ranges} "
                    f"date ranges."
                )

            request_body = {
                "dateRanges": date_ranges,
                "dimensions": [{"name": dim} for dim in dimensions],
                "metrics": [{"name": metric} for metric in metrics],
                "limit": page_size,
                "offset": 0,
            }

            if "date" in dimensions:
                request_body["orderBys"] = [
                    {"dimension": {"dimensionName": "date"}, "desc": False}
                ]

            self._add_optional_filters(request_body, table_options)
            return request_body

        @staticmethod
        def _add_optional_filters(request_body, table_options):
            """Add optional dimension and metric filters to request body."""
            dimension_filter_json = table_options.get("dimension_filter")
            if dimension_filter_json:
                try:
                    request_body["dimensionFilter"] = json.loads(
                        dimension_filter_json
                    )
                except json.JSONDecodeError:
                    raise ValueError(
                        f"Invalid JSON in 'dimension_filter': "
                        f"{dimension_filter_json}"
                    )

            metric_filter_json = table_options.get("metric_filter")
            if metric_filter_json:
                try:
                    request_body["metricFilter"] = json.loads(
                        metric_filter_json
                    )
                except json.JSONDecodeError:
                    raise ValueError(
                        f"Invalid JSON in 'metric_filter': "
                        f"{metric_filter_json}"
                    )

        def _parse_api_row(
            self, row, dimension_headers, metric_headers, property_id
        ):
            """Parse a single API response row into a record dict.

            Returns (record, date_string_or_None).
            """
            record = {"property_id": property_id}
            row_date = None
            date_dims = {"date", "firstSessionDate"}

            dimension_values = row.get("dimensionValues", [])
            for i, dim_header in enumerate(dimension_headers):
                dim_name = dim_header["name"]
                dim_value = (
                    dimension_values[i]["value"]
                    if i < len(dimension_values) else None
                )

                is_parseable_date = (
                    dim_name in date_dims
                    and dim_value and len(dim_value) == 8
                )
                parsed = (
                    self._parse_date_value(dim_value)
                    if is_parseable_date else None
                )
                if parsed:
                    record[dim_name] = parsed
                    if dim_name == "date":
                        row_date = parsed
                else:
                    record[dim_name] = dim_value

            metric_values = row.get("metricValues", [])
            for i, metric_header in enumerate(metric_headers):
                metric_name = metric_header["name"]
                metric_type = metric_header.get("type", "TYPE_STRING")
                value_str = (
                    metric_values[i]["value"]
                    if i < len(metric_values) else None
                )

                if value_str is None or value_str == "":
                    record[metric_name] = None
                else:
                    record[metric_name] = self._parse_metric_value(
                        value_str, metric_type
                    )

            return record, row_date

        def _fetch_property_data(self, property_id, request_body, page_size):
            """Fetch all pages of report data for a single property."""
            request_body = {**request_body}
            rows_list = []
            max_date = None
            offset = 0

            while True:
                request_body["offset"] = offset
                response = self._make_api_request(
                    "runReport", request_body, property_id
                )

                dimension_headers = response.get("dimensionHeaders", [])
                metric_headers = response.get("metricHeaders", [])
                rows = response.get("rows", [])

                if not rows:
                    break

                for row in rows:
                    record, row_date = self._parse_api_row(
                        row, dimension_headers, metric_headers, property_id
                    )
                    rows_list.append(record)
                    if row_date and (
                        max_date is None or row_date > max_date
                    ):
                        max_date = row_date

                if len(rows) < page_size:
                    break

                offset += page_size

            return rows_list, max_date

        def _fetch_report_data(self, request_body, page_size):
            """Fetch report data from all configured properties."""
            all_rows = []
            max_date = None

            for property_id in self.property_ids:
                property_rows, property_max_date = (
                    self._fetch_property_data(
                        property_id, request_body, page_size
                    )
                )
                all_rows.extend(property_rows)
                if property_max_date and (
                    max_date is None or property_max_date > max_date
                ):
                    max_date = property_max_date

            return all_rows, max_date

        def read_table(
            self, table_name: str, start_offset: dict,
            table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records
            and an offset.

            Supports prebuilt reports (by table_name) and custom reports
            (via table_options with dimensions, metrics, filters, etc.).
            """
            table_options = self._get_effective_options(
                table_name, table_options, merge_overrides=True
            )
            dimensions, metrics = self._parse_dimensions_and_metrics(
                table_options
            )
            self._validate_dimensions_and_metrics(dimensions, metrics)

            request_body = self._build_report_request(
                dimensions, metrics, table_options, start_offset
            )

            all_rows, max_date = self._fetch_report_data(
                request_body, request_body["limit"]
            )

            if max_date:
                next_offset = {"last_date": max_date}
            else:
                next_offset = start_offset if start_offset else {}

            return iter(all_rows), next_offset


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = GoogleAnalyticsAggregatedLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)  # pylint: disable=abstract-class-instantiated

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
