# ==============================================================================
# Merged Lakeflow Source: surveymonkey
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from abc import ABC, abstractmethod
from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Tuple,
)
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect(ABC):
        """Base interface that each source connector must implement.

        Subclass this and implement all abstract methods to create a connector that
        integrates with the community connector library and ingestion pipeline.
        """

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """
            self.options = options

        @abstractmethod
        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        @abstractmethod
        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        @abstractmethod
        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        @abstractmethod
        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the records of a table and return an iterator of records and an offset.

            The framework calls this method repeatedly to paginate through data.
            start_offset is None only on the very first call of the very first run
            of a connector. On subsequent runs the framework resumes from the last
            checkpointed offset, so start_offset will already be populated. Each
            call returns (records, end_offset). The framework passes end_offset as
            start_offset to the next call. Pagination stops when the returned
            offset equals start_offset (i.e., no more data).

            For tables that cannot be incrementally read, return None as the offset to
            read the entire table in one batch. Non-checkpointable synthetic offsets can
            be used to split the data into multiple batches.

            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from. None only on the
                    first call of the first run; on subsequent runs it carries the
                    checkpointed offset from the previous run.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of records as JSON-compatible dicts. Do NOT convert
                    values according to get_table_schema(); the framework handles that.
                offset: A dict representing the position after this batch.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            This method follows the same pagination and offset protocol as read_table:
            the framework calls it repeatedly, passing the previous end_offset as
            start_offset, until the returned offset equals start_offset.

            Override this method if any of your tables use ingestion_type
            "cdc_with_deletes". The default implementation raises NotImplementedError.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                A two-element tuple of (records, offset).
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: A dict (same format as read_table).
            """
            raise NotImplementedError(
                "read_table_deletes() must be implemented when ingestion_type is 'cdc_with_deletes'"
            )


    ########################################################
    # src/databricks/labs/community_connector/sources/surveymonkey/surveymonkey_schemas.py
    ########################################################

    BUTTONS_TEXT_STRUCT = StructType(
        [
            StructField("next_button", StringType(), True),
            StructField("prev_button", StringType(), True),
            StructField("done_button", StringType(), True),
            StructField("exit_button", StringType(), True),
        ]
    )

    CONTACT_STRUCT = StructType(
        [
            StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True),
            StructField("email", StringType(), True),
        ]
    )

    METADATA_STRUCT = StructType(
        [
            StructField("contact", CONTACT_STRUCT, True),
        ]
    )

    CHOICE_METADATA_STRUCT = StructType(
        [
            StructField("weight", LongType(), True),
        ]
    )

    ANSWER_STRUCT = StructType(
        [
            StructField("choice_id", StringType(), True),
            StructField("choice_metadata", CHOICE_METADATA_STRUCT, True),
            StructField("row_id", StringType(), True),
            StructField("col_id", StringType(), True),
            StructField("other_id", StringType(), True),
            StructField("text", StringType(), True),
            StructField("download_url", StringType(), True),
            StructField("content_type", StringType(), True),
            StructField("tag_data", ArrayType(StringType()), True),
        ]
    )

    RESPONSE_QUESTION_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("variable_id", StringType(), True),
            StructField("answers", ArrayType(ANSWER_STRUCT), True),
        ]
    )

    RESPONSE_PAGE_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("questions", ArrayType(RESPONSE_QUESTION_STRUCT), True),
        ]
    )

    LOGIC_PATH_STRUCT = StructType(
        [
            StructField("path", ArrayType(StringType()), True),
        ]
    )

    CHOICE_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("position", LongType(), True),
            StructField("text", StringType(), True),
            StructField("visible", BooleanType(), True),
            StructField("is_na", BooleanType(), True),
            StructField("weight", LongType(), True),
        ]
    )

    OTHER_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("text", StringType(), True),
            StructField("visible", BooleanType(), True),
            StructField("is_answer_choice", BooleanType(), True),
            StructField("position", LongType(), True),
        ]
    )

    QUESTION_ANSWERS_STRUCT = StructType(
        [
            StructField("choices", ArrayType(CHOICE_STRUCT), True),
            StructField("rows", ArrayType(CHOICE_STRUCT), True),
            StructField("cols", ArrayType(CHOICE_STRUCT), True),
            StructField("other", OTHER_STRUCT, True),
        ]
    )

    REQUIRED_STRUCT = StructType(
        [
            StructField("text", StringType(), True),
            StructField("type", StringType(), True),
            StructField("amount", StringType(), True),
        ]
    )

    SORTING_STRUCT = StructType(
        [
            StructField("type", StringType(), True),
            StructField("ignore_last", BooleanType(), True),
        ]
    )

    VALIDATION_STRUCT = StructType(
        [
            StructField("type", StringType(), True),
            StructField("text", StringType(), True),
            StructField("min", StringType(), True),
            StructField("max", StringType(), True),
            StructField("sum", LongType(), True),
            StructField("sum_text", StringType(), True),
        ]
    )

    THANK_YOU_PAGE_STRUCT = StructType(
        [
            StructField("is_enabled", BooleanType(), True),
            StructField("message", StringType(), True),
        ]
    )

    WORKGROUP_METADATA_STRUCT = StructType(
        [
            StructField("key", StringType(), True),
            StructField("value", StringType(), True),
        ]
    )

    DEFAULT_ROLE_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
        ]
    )

    WORKGROUP_MEMBER_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("email", StringType(), True),
        ]
    )

    ROLLUP_SUMMARY_STRUCT = StructType(
        [
            StructField("text", StringType(), True),
            StructField("choice_id", StringType(), True),
            StructField("count", LongType(), True),
            StructField("percentage", DoubleType(), True),
            StructField("other_text", ArrayType(StringType()), True),
            StructField("min", DoubleType(), True),
            StructField("max", DoubleType(), True),
            StructField("mean", DoubleType(), True),
            StructField("median", DoubleType(), True),
        ]
    )

    BENCHMARK_DETAILS_STRUCT = StructType(
        [
            StructField("industry", StringType(), True),
            StructField("sample_size", LongType(), True),
        ]
    )

    QUESTION_TYPES_STRUCT = StructType(
        [
            StructField("available", ArrayType(StringType()), True),
        ]
    )

    SCOPES_STRUCT = StructType(
        [
            StructField("available", ArrayType(StringType()), True),
            StructField("granted", ArrayType(StringType()), True),
        ]
    )

    FEATURES_STRUCT = StructType(
        [
            StructField("collector_create_limit", LongType(), True),
            StructField("email_enabled", BooleanType(), True),
        ]
    )

    # ─── Table Schemas ────────────────────────────────────────────────────────────

    TABLE_SCHEMAS = {
        "surveys": StructType(
            [
                StructField("id", StringType(), False),
                StructField("title", StringType(), True),
                StructField("nickname", StringType(), True),
                StructField("href", StringType(), True),
                StructField("folder_id", StringType(), True),
                StructField("category", StringType(), True),
                StructField("language", StringType(), True),
                StructField("question_count", LongType(), True),
                StructField("page_count", LongType(), True),
                StructField("date_created", StringType(), True),
                StructField("date_modified", StringType(), True),
                StructField("response_count", LongType(), True),
                StructField("buttons_text", BUTTONS_TEXT_STRUCT, True),
                StructField("is_owner", BooleanType(), True),
                StructField("footer", BooleanType(), True),
                StructField("custom_variables", MapType(StringType(), StringType()), True),
                StructField("preview", StringType(), True),
                StructField("edit_url", StringType(), True),
                StructField("collect_url", StringType(), True),
                StructField("analyze_url", StringType(), True),
                StructField("summary_url", StringType(), True),
            ]
        ),
        "survey_responses": StructType(
            [
                StructField("id", StringType(), False),
                StructField("survey_id", StringType(), True),
                StructField("recipient_id", StringType(), True),
                StructField("collector_id", StringType(), True),
                StructField("custom_value", StringType(), True),
                StructField("edit_url", StringType(), True),
                StructField("analyze_url", StringType(), True),
                StructField("total_time", LongType(), True),
                StructField("date_created", StringType(), True),
                StructField("date_modified", StringType(), True),
                StructField("response_status", StringType(), True),
                StructField("collection_mode", StringType(), True),
                StructField("ip_address", StringType(), True),
                StructField("logic_path", LOGIC_PATH_STRUCT, True),
                StructField("metadata", METADATA_STRUCT, True),
                StructField("page_path", ArrayType(StringType()), True),
                StructField("pages", ArrayType(RESPONSE_PAGE_STRUCT), True),
            ]
        ),
        "survey_pages": StructType(
            [
                StructField("id", StringType(), False),
                StructField("survey_id", StringType(), True),
                StructField("title", StringType(), True),
                StructField("description", StringType(), True),
                StructField("position", LongType(), True),
                StructField("question_count", LongType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "survey_questions": StructType(
            [
                StructField("id", StringType(), False),
                StructField("survey_id", StringType(), True),
                StructField("page_id", StringType(), True),
                StructField("position", LongType(), True),
                StructField("visible", BooleanType(), True),
                StructField("family", StringType(), True),
                StructField("subtype", StringType(), True),
                StructField("heading", StringType(), True),
                StructField("href", StringType(), True),
                StructField("sorting", SORTING_STRUCT, True),
                StructField("required", REQUIRED_STRUCT, True),
                StructField("validation", VALIDATION_STRUCT, True),
                StructField("forced_ranking", BooleanType(), True),
                StructField("answers", QUESTION_ANSWERS_STRUCT, True),
            ]
        ),
        "collectors": StructType(
            [
                StructField("id", StringType(), False),
                StructField("survey_id", StringType(), True),
                StructField("name", StringType(), True),
                StructField("type", StringType(), True),
                StructField("status", StringType(), True),
                StructField("redirect_url", StringType(), True),
                StructField("thank_you_page", THANK_YOU_PAGE_STRUCT, True),
                StructField("response_count", LongType(), True),
                StructField("date_created", StringType(), True),
                StructField("date_modified", StringType(), True),
                StructField("url", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "contact_lists": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "contacts": StructType(
            [
                StructField("id", StringType(), False),
                StructField("email", StringType(), True),
                StructField("first_name", StringType(), True),
                StructField("last_name", StringType(), True),
                StructField("custom_fields", MapType(StringType(), StringType()), True),
                StructField("href", StringType(), True),
            ]
        ),
        "users": StructType(
            [
                StructField("id", StringType(), False),
                StructField("username", StringType(), True),
                StructField("first_name", StringType(), True),
                StructField("last_name", StringType(), True),
                StructField("email", StringType(), True),
                StructField("email_verified", BooleanType(), True),
                StructField("account_type", StringType(), True),
                StructField("language", StringType(), True),
                StructField("date_created", StringType(), True),
                StructField("date_last_login", StringType(), True),
                StructField("question_types", QUESTION_TYPES_STRUCT, True),
                StructField("scopes", SCOPES_STRUCT, True),
                StructField("sso_connections", ArrayType(StringType()), True),
                StructField("features", FEATURES_STRUCT, True),
                StructField("href", StringType(), True),
            ]
        ),
        "groups": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("member_count", LongType(), True),
                StructField("max_invites", LongType(), True),
                StructField("date_created", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "group_members": StructType(
            [
                StructField("id", StringType(), False),
                StructField("group_id", StringType(), True),
                StructField("username", StringType(), True),
                StructField("email", StringType(), True),
                StructField("type", StringType(), True),
                StructField("status", StringType(), True),
                StructField("date_created", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "workgroups": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("description", StringType(), True),
                StructField("is_visible", BooleanType(), True),
                StructField("metadata", WORKGROUP_METADATA_STRUCT, True),
                StructField("created_at", StringType(), True),
                StructField("updated_at", StringType(), True),
                StructField("members", ArrayType(WORKGROUP_MEMBER_STRUCT), True),
                StructField("members_count", LongType(), True),
                StructField("shares_count", LongType(), True),
                StructField("default_role", DEFAULT_ROLE_STRUCT, True),
                StructField("organization_id", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "survey_folders": StructType(
            [
                StructField("id", StringType(), False),
                StructField("title", StringType(), True),
                StructField("num_surveys", LongType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "survey_categories": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
            ]
        ),
        "survey_templates": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("title", StringType(), True),
                StructField("description", StringType(), True),
                StructField("category", StringType(), True),
                StructField("available", BooleanType(), True),
                StructField("num_questions", LongType(), True),
                StructField("preview_link", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "survey_languages": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("native_name", StringType(), True),
            ]
        ),
        "webhooks": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("event_type", StringType(), True),
                StructField("object_type", StringType(), True),
                StructField("object_ids", ArrayType(StringType()), True),
                StructField("subscription_url", StringType(), True),
                StructField("authorization", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
        "survey_rollups": StructType(
            [
                StructField("id", StringType(), False),
                StructField("survey_id", StringType(), True),
                StructField("family", StringType(), True),
                StructField("subtype", StringType(), True),
                StructField("href", StringType(), True),
                StructField("summary", ArrayType(MapType(StringType(), StringType())), True),
            ]
        ),
        "benchmark_bundles": StructType(
            [
                StructField("id", StringType(), False),
                StructField("name", StringType(), True),
                StructField("description", StringType(), True),
                StructField("title", StringType(), True),
                StructField("details", BENCHMARK_DETAILS_STRUCT, True),
                StructField("country_code", StringType(), True),
                StructField("href", StringType(), True),
            ]
        ),
    }

    # ─── Table Metadata ──────────────────────────────────────────────────────────

    OBJECT_CONFIG = {
        "surveys": {
            "primary_keys": ["id"],
            "cursor_field": "date_modified",
            "ingestion_type": "cdc",
            "endpoint": "/surveys",
            "per_page": 1000,
        },
        "survey_responses": {
            "primary_keys": ["id"],
            "cursor_field": "date_modified",
            "ingestion_type": "cdc",
            "endpoint": "/surveys/{survey_id}/responses/bulk",
            "per_page": 100,
            "requires_survey_id": True,
        },
        "survey_pages": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/surveys/{survey_id}/pages",
            "per_page": 1000,
            "requires_survey_id": True,
        },
        "survey_questions": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/surveys/{survey_id}/pages/{page_id}/questions",
            "per_page": 1000,
            "requires_survey_id": True,
            "requires_page_id": True,
        },
        "collectors": {
            "primary_keys": ["id"],
            "cursor_field": "date_modified",
            "ingestion_type": "cdc",
            "endpoint": "/surveys/{survey_id}/collectors",
            "per_page": 1000,
            "requires_survey_id": True,
        },
        "contact_lists": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/contact_lists",
            "per_page": 1000,
        },
        "contacts": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/contacts",
            "per_page": 1000,
        },
        "users": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/users/me",
            "per_page": 1,
        },
        "groups": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/groups",
            "per_page": 1000,
        },
        "group_members": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/groups/{group_id}/members",
            "per_page": 1000,
            "requires_group_id": True,
        },
        "workgroups": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/workgroups",
            "per_page": 1000,
        },
        "survey_folders": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/survey_folders",
            "per_page": 1000,
        },
        "survey_categories": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/survey_categories",
            "per_page": 1000,
        },
        "survey_templates": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/survey_templates",
            "per_page": 1000,
        },
        "survey_languages": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/survey_languages",
            "per_page": 1000,
        },
        "webhooks": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/webhooks",
            "per_page": 1000,
        },
        "survey_rollups": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/surveys/{survey_id}/rollups",
            "per_page": 100,
            "requires_survey_id": True,
        },
        "benchmark_bundles": {
            "primary_keys": ["id"],
            "cursor_field": None,
            "ingestion_type": "snapshot",
            "endpoint": "/benchmark_bundles",
            "per_page": 1000,
        },
    }

    SUPPORTED_TABLES = list(TABLE_SCHEMAS.keys())


    ########################################################
    # src/databricks/labs/community_connector/sources/surveymonkey/surveymonkey.py
    ########################################################

    class SurveymonkeyLakeflowConnect(LakeflowConnect):
        def __init__(self, options: dict) -> None:
            """
            Initialize the SurveyMonkey connector with API credentials.

            Args:
                options: Dictionary containing:
                    - access_token: SurveyMonkey OAuth access token
                    - base_url (optional): API base URL, defaults to US data center
            """
            self.access_token = options["access_token"]
            # Support both US and EU data centers
            self.base_url = options.get("base_url", "https://api.surveymonkey.com/v3")
            self.headers = {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json",
            }

        # ─── Interface Methods ────────────────────────────────────────────────────

        def list_tables(self) -> List[str]:
            """List available SurveyMonkey tables/objects."""
            return SUPPORTED_TABLES.copy()

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """Get the Spark schema for a SurveyMonkey table."""
            self._validate_table(table_name)
            return TABLE_SCHEMAS[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """Get metadata for a SurveyMonkey table."""
            self._validate_table(table_name)
            config = OBJECT_CONFIG[table_name]
            result = {
                "primary_keys": config["primary_keys"],
                "ingestion_type": config["ingestion_type"],
            }
            # Only include cursor_field for cdc ingestion type
            if config["ingestion_type"] == "cdc" and config["cursor_field"]:
                result["cursor_field"] = config["cursor_field"]
            return result

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read data from a SurveyMonkey table."""
            self._validate_table(table_name)

            config = OBJECT_CONFIG[table_name]

            # Validate required parameters (some tables can iterate over all parents if not provided)
            requires_survey_id = config.get("requires_survey_id", False)
            requires_page_id = config.get("requires_page_id", False)
            requires_group_id = config.get("requires_group_id", False)

            # Tables that can iterate over all parent objects if parent_id not provided
            can_iterate_surveys = table_name in [
                "survey_responses",
                "survey_pages",
                "survey_questions",
                "collectors",
                "survey_rollups",
            ]
            can_iterate_pages = table_name in ["survey_questions"]
            can_iterate_groups = table_name in ["group_members"]

            if requires_survey_id and not table_options.get("survey_id") and not can_iterate_surveys:
                raise ValueError(
                    f"Table '{table_name}' requires 'survey_id' in table_options"
                )
            if requires_page_id and not table_options.get("page_id") and not can_iterate_pages:
                raise ValueError(
                    f"Table '{table_name}' requires 'page_id' in table_options"
                )
            if requires_group_id and not table_options.get("group_id") and not can_iterate_groups:
                raise ValueError(
                    f"Table '{table_name}' requires 'group_id' in table_options"
                )

            # Determine ingestion type and read accordingly
            if config["ingestion_type"] == "cdc":
                cursor_field = config["cursor_field"]
                is_incremental = (
                    start_offset is not None
                    and start_offset.get(cursor_field) is not None
                )
                if is_incremental:
                    return self._read_data_incremental(table_name, start_offset, table_options)
                else:
                    return self._read_data_full(table_name, table_options)
            else:
                # Snapshot ingestion
                return self._read_data_full(table_name, table_options)

        # ─── Helpers ──────────────────────────────────────────────────────────────

        def _validate_table(self, table_name: str) -> None:
            """Validate that the table is supported."""
            if table_name not in TABLE_SCHEMAS:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {SUPPORTED_TABLES}"
                )

        def _make_request(
            self, url: str, params: dict = None, retries: int = 3, allow_empty_on_error: bool = False
        ) -> dict:
            """Make an API request with retry logic and rate limit handling."""
            for attempt in range(retries):
                response = requests.get(url, headers=self.headers, params=params)

                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    # Rate limited - wait and retry
                    reset_time = int(response.headers.get("X-Ratelimit-App-Global-Minute-Reset", 60))
                    time.sleep(min(reset_time, 60))
                    continue
                elif response.status_code in (400, 403) and allow_empty_on_error:
                    # Permission denied or bad request - return empty data
                    return {"data": []}
                else:
                    raise Exception(
                        f"SurveyMonkey API error: {response.status_code} {response.text}"
                    )

            raise Exception("Max retries exceeded due to rate limiting")

        def _build_endpoint_url(self, table_name: str, table_options: Dict[str, str]) -> str:
            """Build the API endpoint URL with path parameters substituted."""
            config = OBJECT_CONFIG[table_name]
            endpoint = config["endpoint"]

            # Substitute path parameters
            if "{survey_id}" in endpoint:
                endpoint = endpoint.replace("{survey_id}", table_options["survey_id"])
            if "{page_id}" in endpoint:
                endpoint = endpoint.replace("{page_id}", table_options["page_id"])
            if "{group_id}" in endpoint:
                endpoint = endpoint.replace("{group_id}", table_options["group_id"])

            return f"{self.base_url}{endpoint}"

        def _clean_empty_dicts(self, obj):
            """Recursively convert empty dicts to None for nested structures."""
            if isinstance(obj, dict):
                if not obj:
                    return None
                return {k: self._clean_empty_dicts(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [self._clean_empty_dicts(item) for item in obj]
            return obj

        def _add_parent_identifiers(
            self, table_name: str, record: dict, table_options: Dict[str, str]
        ) -> dict:
            """Add parent identifiers to child records."""
            config = OBJECT_CONFIG[table_name]

            if config.get("requires_survey_id"):
                record["survey_id"] = table_options.get("survey_id")
            if config.get("requires_page_id"):
                record["page_id"] = table_options.get("page_id")
            if config.get("requires_group_id"):
                record["group_id"] = table_options.get("group_id")

            return record

        # ─── Table Readers ────────────────────────────────────────────────────────

        def _get_special_handler(self, table_name: str, table_options: Dict[str, str]):
            """Return a special handler for a table, or None."""
            handlers = {
                "users": lambda: self._read_single_user(),
                "survey_pages": lambda: self._read_survey_pages(table_options),
                "survey_questions": lambda: (self._read_all_survey_questions(table_options)),
                "survey_rollups": lambda: self._read_survey_rollups(table_options),
            }

            if table_name in handlers:
                return handlers[table_name]

            # Conditional handlers that depend on missing parent IDs
            conditional = {
                "survey_responses": (
                    "survey_id",
                    lambda: self._read_all_survey_responses(table_options, start_modified_at=None),
                ),
                "collectors": (
                    "survey_id",
                    lambda: self._read_all_collectors(table_options),
                ),
                "group_members": (
                    "group_id",
                    lambda: self._read_all_group_members(table_options),
                ),
            }

            if table_name in conditional:
                parent_key, handler = conditional[table_name]
                if not table_options.get(parent_key):
                    return handler

            return None

        def _read_data_full(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read all data from a SurveyMonkey table (full refresh)."""
            config = OBJECT_CONFIG[table_name]

            # Dispatch to special handlers for specific tables
            handler = self._get_special_handler(table_name, table_options)
            if handler:
                return handler()

            url = self._build_endpoint_url(table_name, table_options)
            per_page = config["per_page"]

            all_records = []
            latest_cursor_value = None
            page = 1

            while True:
                params = {
                    "page": page,
                    "per_page": per_page,
                }

                # For surveys, request additional fields
                if table_name == "surveys":
                    params["include"] = (
                        "response_count,date_created,date_modified,language,question_count"
                    )
                    params["sort_by"] = "date_modified"
                    params["sort_order"] = "ASC"

                # Some endpoints require special permissions - allow empty results on 400/403
                allow_empty = table_name in ["workgroups", "webhooks", "benchmark_bundles", "groups"]
                data = self._make_request(url, params, allow_empty_on_error=allow_empty)

                # Handle single record response (like /users/me)
                if "data" not in data:
                    data = self._clean_empty_dicts(data)
                    return iter([data]), {}

                # Handle list response
                records = data.get("data", [])
                if not records:
                    break

                # Add parent identifiers for child objects and clean empty dicts
                for record in records:
                    record = self._add_parent_identifiers(table_name, record, table_options)
                    record = self._clean_empty_dicts(record)
                    all_records.append(record)

                    # Track latest cursor value for cdc tables
                    if config["ingestion_type"] == "cdc" and config["cursor_field"]:
                        cursor_value = record.get(config["cursor_field"])
                        if cursor_value:
                            if latest_cursor_value is None or cursor_value > latest_cursor_value:
                                latest_cursor_value = cursor_value

                # Check for more pages
                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)  # Rate limiting

            # Build offset
            offset = {}
            if config["ingestion_type"] == "cdc" and config["cursor_field"] and latest_cursor_value:
                offset[config["cursor_field"]] = latest_cursor_value

            return iter(all_records), offset

        def _read_data_incremental(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read incremental data from a SurveyMonkey table."""
            config = OBJECT_CONFIG[table_name]
            cursor_field = config["cursor_field"]
            cursor_start = start_offset.get(cursor_field)

            # Handle special case for survey_responses
            if table_name == "survey_responses" and not table_options.get("survey_id"):
                return self._read_all_survey_responses(table_options, start_modified_at=cursor_start)

            # Handle special case for collectors across all surveys
            if table_name == "collectors" and not table_options.get("survey_id"):
                return self._read_all_collectors(table_options, start_modified_at=cursor_start)

            url = self._build_endpoint_url(table_name, table_options)
            per_page = config["per_page"]

            all_records = []
            latest_cursor_value = cursor_start
            page = 1

            while True:
                params = {
                    "page": page,
                    "per_page": per_page,
                    "sort_by": cursor_field,
                    "sort_order": "ASC",
                }

                # Add incremental filter
                if cursor_start:
                    params["start_modified_at"] = cursor_start

                # For surveys, request additional fields
                if table_name == "surveys":
                    params["include"] = (
                        "response_count,date_created,date_modified,language,question_count"
                    )

                data = self._make_request(url, params)
                records = data.get("data", [])

                if not records:
                    break

                for record in records:
                    record = self._add_parent_identifiers(table_name, record, table_options)
                    record = self._clean_empty_dicts(record)
                    all_records.append(record)

                    # Track latest cursor value
                    cursor_value = record.get(cursor_field)
                    if cursor_value:
                        if latest_cursor_value is None or cursor_value > latest_cursor_value:
                            latest_cursor_value = cursor_value

                # Check for more pages
                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            offset = {cursor_field: latest_cursor_value} if latest_cursor_value else {}
            return iter(all_records), offset

        def _read_single_user(self) -> Tuple[Iterator[dict], dict]:
            """Read the current user's information."""
            url = f"{self.base_url}/users/me"
            data = self._make_request(url)
            data = self._clean_empty_dicts(data)
            return iter([data]), {}

        def _read_all_survey_responses(
            self, table_options: Dict[str, str], start_modified_at: str = None
        ) -> Tuple[Iterator[dict], dict]:
            """Read responses across all surveys."""
            # First, get all surveys
            surveys_url = f"{self.base_url}/surveys"
            all_surveys = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                data = self._make_request(surveys_url, params)
                surveys = data.get("data", [])

                if not surveys:
                    break

                all_surveys.extend(surveys)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Then, get responses for each survey
            all_responses = []
            latest_cursor_value = start_modified_at

            for survey in all_surveys:
                survey_id = survey["id"]
                responses_url = f"{self.base_url}/surveys/{survey_id}/responses/bulk"
                page = 1

                while True:
                    params = {
                        "page": page,
                        "per_page": 100,
                        "sort_by": "date_modified",
                        "sort_order": "ASC",
                    }

                    if start_modified_at:
                        params["start_modified_at"] = start_modified_at

                    try:
                        data = self._make_request(responses_url, params)
                    except Exception:
                        # Skip surveys with no access or errors
                        break

                    responses = data.get("data", [])

                    if not responses:
                        break

                    for response in responses:
                        response["survey_id"] = survey_id
                        response = self._clean_empty_dicts(response)
                        all_responses.append(response)

                        cursor_value = response.get("date_modified")
                        if cursor_value:
                            if latest_cursor_value is None or cursor_value > latest_cursor_value:
                                latest_cursor_value = cursor_value

                    if "next" not in data.get("links", {}):
                        break

                    page += 1
                    time.sleep(0.1)

            offset = {"date_modified": latest_cursor_value} if latest_cursor_value else {}
            return iter(all_responses), offset

        def _read_all_survey_questions(
            self, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read all questions from a survey by iterating through pages."""
            survey_id = table_options.get("survey_id")

            # If no survey_id provided, iterate over all surveys
            if not survey_id:
                return self._read_all_questions_all_surveys()

            # Use the /surveys/{id}/details endpoint to get pages and questions
            details_url = f"{self.base_url}/surveys/{survey_id}/details"
            all_questions = []

            try:
                data = self._make_request(details_url)
                pages = data.get("pages", [])

                for survey_page in pages:
                    page_id = survey_page["id"]
                    questions = survey_page.get("questions", [])

                    for question in questions:
                        question["survey_id"] = survey_id
                        question["page_id"] = page_id
                        question = self._clean_empty_dicts(question)
                        all_questions.append(question)
            except Exception:
                pass

            return iter(all_questions), {}

        def _read_all_questions_all_surveys(self) -> Tuple[Iterator[dict], dict]:
            """Read all questions from all surveys."""
            # First, get all surveys
            surveys_url = f"{self.base_url}/surveys"
            all_surveys = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                data = self._make_request(surveys_url, params)
                surveys = data.get("data", [])

                if not surveys:
                    break

                all_surveys.extend(surveys)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Then, get questions for each survey via details endpoint
            all_questions = []

            for survey in all_surveys:
                survey_id = survey["id"]
                details_url = f"{self.base_url}/surveys/{survey_id}/details"

                try:
                    data = self._make_request(details_url)
                    pages = data.get("pages", [])

                    for survey_page in pages:
                        page_id = survey_page["id"]
                        questions = survey_page.get("questions", [])

                        for question in questions:
                            question["survey_id"] = survey_id
                            question["page_id"] = page_id
                            question = self._clean_empty_dicts(question)
                            all_questions.append(question)
                except Exception:
                    # Skip surveys with no access or errors
                    continue

                time.sleep(0.1)

            return iter(all_questions), {}

        def _read_survey_pages(
            self, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read pages from a specific survey or all surveys."""
            survey_id = table_options.get("survey_id")

            if survey_id:
                # Read pages for a specific survey
                details_url = f"{self.base_url}/surveys/{survey_id}/details"
                all_pages = []

                try:
                    data = self._make_request(details_url)
                    pages = data.get("pages", [])

                    for survey_page in pages:
                        survey_page["survey_id"] = survey_id
                        survey_page = self._clean_empty_dicts(survey_page)
                        all_pages.append(survey_page)
                except Exception:
                    pass

                return iter(all_pages), {}

            # No survey_id - read pages from all surveys
            surveys_url = f"{self.base_url}/surveys"
            all_surveys = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                data = self._make_request(surveys_url, params)
                surveys = data.get("data", [])

                if not surveys:
                    break

                all_surveys.extend(surveys)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Get pages for each survey using the details endpoint
            all_pages = []

            for survey in all_surveys:
                sid = survey["id"]
                details_url = f"{self.base_url}/surveys/{sid}/details"

                try:
                    data = self._make_request(details_url)
                    pages = data.get("pages", [])

                    for survey_page in pages:
                        survey_page["survey_id"] = sid
                        survey_page = self._clean_empty_dicts(survey_page)
                        all_pages.append(survey_page)
                except Exception:
                    # Skip surveys with no access or errors
                    continue

                time.sleep(0.1)

            return iter(all_pages), {}

        def _read_all_collectors(
            self, table_options: Dict[str, str], start_modified_at: str = None
        ) -> Tuple[Iterator[dict], dict]:
            """Read all collectors from all surveys."""
            # First, get all surveys
            surveys_url = f"{self.base_url}/surveys"
            all_surveys = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                data = self._make_request(surveys_url, params)
                surveys = data.get("data", [])

                if not surveys:
                    break

                all_surveys.extend(surveys)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Then, get collectors for each survey
            all_collectors = []
            latest_cursor_value = start_modified_at

            for survey in all_surveys:
                survey_id = survey["id"]
                collectors_url = f"{self.base_url}/surveys/{survey_id}/collectors"
                page = 1

                while True:
                    params = {
                        "page": page,
                        "per_page": 1000,
                        "sort_by": "date_modified",
                        "sort_order": "ASC",
                    }

                    if start_modified_at:
                        params["start_modified_at"] = start_modified_at

                    try:
                        data = self._make_request(collectors_url, params)
                    except Exception:
                        # Skip surveys with no access or errors
                        break

                    collectors = data.get("data", [])

                    if not collectors:
                        break

                    for collector in collectors:
                        collector["survey_id"] = survey_id
                        collector = self._clean_empty_dicts(collector)
                        all_collectors.append(collector)

                        cursor_value = collector.get("date_modified")
                        if cursor_value:
                            if latest_cursor_value is None or cursor_value > latest_cursor_value:
                                latest_cursor_value = cursor_value

                    if "next" not in data.get("links", {}):
                        break

                    page += 1
                    time.sleep(0.1)

            offset = {"date_modified": latest_cursor_value} if latest_cursor_value else {}
            return iter(all_collectors), offset

        def _read_all_group_members(
            self, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read all members from all groups."""
            # First, get all groups
            groups_url = f"{self.base_url}/groups"
            all_groups = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                try:
                    data = self._make_request(groups_url, params)
                except Exception:
                    # Groups endpoint may not be available for all accounts
                    break

                groups = data.get("data", [])

                if not groups:
                    break

                all_groups.extend(groups)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Then, get members for each group
            all_members = []

            for group in all_groups:
                group_id = group["id"]
                members_url = f"{self.base_url}/groups/{group_id}/members"
                page = 1

                while True:
                    params = {"page": page, "per_page": 1000}

                    try:
                        data = self._make_request(members_url, params)
                    except Exception:
                        # Skip groups with no access or errors
                        break

                    members = data.get("data", [])

                    if not members:
                        break

                    for member in members:
                        member["group_id"] = group_id
                        member = self._clean_empty_dicts(member)
                        all_members.append(member)

                    if "next" not in data.get("links", {}):
                        break

                    page += 1
                    time.sleep(0.1)

            return iter(all_members), {}

        def _read_survey_rollups(
            self, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """Read rollup statistics for a specific survey or all surveys."""
            survey_id = table_options.get("survey_id")

            if survey_id:
                return self._read_rollups_for_survey(survey_id)

            # No survey_id - read rollups from all surveys
            surveys_url = f"{self.base_url}/surveys"
            all_surveys = []
            page = 1

            while True:
                params = {"page": page, "per_page": 1000}
                data = self._make_request(surveys_url, params)
                surveys = data.get("data", [])

                if not surveys:
                    break

                all_surveys.extend(surveys)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            # Get rollups for each survey
            all_rollups = []

            for survey in all_surveys:
                sid = survey["id"]
                rollups, _ = self._read_rollups_for_survey(sid)
                all_rollups.extend(list(rollups))
                time.sleep(0.1)

            return iter(all_rollups), {}

        def _read_rollups_for_survey(
            self, survey_id: str
        ) -> Tuple[Iterator[dict], dict]:
            """Read rollup statistics for a specific survey."""
            rollups_url = f"{self.base_url}/surveys/{survey_id}/rollups"
            all_rollups = []
            page = 1

            while True:
                params = {"page": page, "per_page": 100}

                try:
                    data = self._make_request(rollups_url, params)
                except Exception:
                    # Skip surveys with no access or errors
                    break

                rollups = data.get("data", [])

                if not rollups:
                    break

                for rollup in rollups:
                    rollup["survey_id"] = survey_id
                    rollup = self._clean_empty_dicts(rollup)
                    all_rollups.append(rollup)

                if "next" not in data.get("links", {}):
                    break

                page += 1
                time.sleep(0.1)

            return iter(all_rollups), {}

        def test_connection(self) -> dict:
            """Test the connection to SurveyMonkey API."""
            try:
                url = f"{self.base_url}/users/me"
                response = requests.get(url, headers=self.headers)

                if response.status_code == 200:
                    user_data = response.json()
                    return {
                        "status": "success",
                        "message": f"Connected as {user_data.get('email', 'unknown')}",
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"API error: {response.status_code} {response.text}",
                    }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = SurveymonkeyLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)  # pylint: disable=abstract-class-instantiated

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
